<!DOCTYPE html>
<html lang="vi">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Sampling techniques for Partially Annotated Detection | Dang-Khoa&#39;s blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/books/">Books</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
      

    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Sampling techniques for Partially Annotated Detection</span></h1>

<h2 class="date">2019/06/11</h2>
</div>



<main>


<p>In this post, I will discuss two papers that try to handle the partially
annotated datasets. Let talk a bit about why we care about missing annotations
in detection. Firstly, labeling box is difficult and tedious. Increasing the size of
taxonomy also exponentially increase the difficulty of the task. Secondly,
Suppose that originally we have a training dataset with 20 categories, later,
we want to add 10 more new categories into the model. The question is: do we
need to reannotate the training dataset? Or do we have any techniques that
automatically solve the problem? Recently, we have the Open Images Dataset
with a huge number of images and annotations, hence, the community also
becomes interested in this problem. I found two papers that are interesting:</p>

<ol>
<li>Wu, Zhe, et al. &ldquo;Soft sampling for robust object detection.&rdquo; arXiv preprint arXiv:1806.06986 (2018).</li>
<li>Niitani, Yusuke, et al. &ldquo;Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects.&rdquo; arXiv preprint arXiv:1811.10862 (2018).</li>
</ol>

<p>In the first paper, the authors try to study the robustness of the object detection
system under the presence of mission annotations. I have done this one before
with COCO-like datasets. However, the authors have a systematic way to
conduct experiments than me. Their conclusion is also interesting:
&gt; we observe that after dropping 30% of the annotations (and labeling them
&gt; as background), the performance of CNN-based object detectors like
&gt; Faster-RCNN only drops by 5% on the PASCAL VOC dataset.</p>

<p>The thing is: the conclusion is drawn when you set the detection threshold at 0.
It is not possible for any real object detections. You have to set higher
thresholds in order to maintain certain precision/recall. I believe that
any commercial object detection systems do that. For that reason, if we look at
the result at a threshold larger than 0.4, we can observe a significant drop
in term of mAP, which makes sense. That being said, the authors also mentioned
in section 4 that &ldquo;it is important for practitioners to tune the detection
threshold per class when using detectors trained on missing labels&rdquo;.</p>

<p>The little game is the second figure of the paper in which they show the
performance changes on <code>trainval</code> and <code>test</code> set of VOC2007 with different
detection thresholds.</p>

<figure>
    <img src="/partial_annotation/missing_annotation_perf.jpg"
         alt="Performance changes on various detection thresholds"/> <figcaption>
            <p>Performance changes on various detection thresholds</p>
        </figcaption>
</figure>

<p>One thing worth noting in the experiment setting is that they drop groundtruth
boxes across all classes, which is quite different compared to the scenario
in which we add more class into the trained model. While the former setting
does not change any taxonomy, the latter revamps the whole label information.</p>

<p>Now, let move on to the proposed method. Firstly, they suggest using hard-mining
example sampling to tackle the missing annotation problem. The reason is that
by mining hard examples, we can avoid randomly sampling the missing annotation
regions. Then, they propose a fancy function that weights the gradient
based on the IoU overlapping value. To this point, you can see that the proposed method is just
another form of <a href="http://dangkhoasdc.github.io/en/post/2019/06/03/wip-libra-r-cnn-paper-review-and-comments/">Balance IoU Sampler</a>.</p>

<p>So, there is nothing surprising here.</p>

<p>The authors also propose another approach (otherwise, the paper is just too
short to appear in any conferences). This time they want to weight
the gradient of ROIs that are <strong>not</strong> positives or hard negatives. The weight
function is akin to the aforementioned weight function. So basically, they
believe their model, hoping that it relatively predict the correct objects.
If the considering ROIs (of course, neither positive nor hard negative ROIs)
has a high score, they consider it as a positive ROI and enforce the gradient,
otherwise just subdue that ROI. They also mentioned that the trained model
is quite weak, hence this approach does not work well. The experiment results
clearly support the observation.</p>

<p>Now, let talk about the second paper. The proposed method is named as <em>pseudo
label-guided</em>. Their observation is that if an object presents in the image,
some parts of the object should be included as well. Say you have a car in a
photo, it probably also has tires in that photo as well. In other words, this
kind of approach is only suitable for hierarchical taxonomies.</p>

<p>The proposed method is composed of two components:</p>

<ol>
<li><p><strong>part-aware sampling</strong>: they simply ignore the classification loss of
part categories when an instance of them is inside an instance of their
subject categories.</p></li>

<li><p><strong>pseudo labels</strong>: to exclude regions that are likely not to be annotated.</p></li>
</ol>

<p>So, basically, they want to ignore those regions they think are missed annotated.</p>

<p>Table 1. in the paper is interesting. There are two notions here:</p>

<ol>
<li>Included: the ratio between a set of part component and subject category in the
same instance and total bounding boxes of the part component.</li>
<li>Co-occur: the ratio between images obtains both part and subject categories and
total images having subject categories.</li>
</ol>

<p>The number suggests that missing annotation are a severe problem in the Open
Images Dataset.</p>

<figure>
    <img src="/partial_annotation/part_subject_stat.jpg"/> 
</figure>

<p>Even though they summarized two algorithms in the paper, it is worth translating
them to English:</p>

<ul>
<li><p><strong>Part-ware sampling</strong>: For each RoI proposal (Line 1), check if the associated
groundtruth (Line 3) contains part categories (Line 4). If it does, ignore
those labels (Line 6) that are not verified (Line 5).</p></li>

<li><p><strong>Pseudo label-guided sampling</strong>: For each output from a trained model (Line 2),
remove those whose score is smaller than the threshold or its label is in the
verified set (Line 3), otherwise remove it if it very close to any
groundtruth (Line 6). After that, for each RoI proposal (Line 8), add those
box from the filtered output to the ignored groups (Line 11) if its IoU with
the RoI is high enough (Line 10).</p></li>
</ul>

<h1 id="experiments">Experiments</h1>

<p>Nothing special here, they just show off their experiment. However, I will
implement <strong>soft sampling</strong> and <strong>pseudo label-guided sampling</strong> in the next
couple of weeks. Let see if these methods can actually help my work or not.</p>

</main>

  <footer>
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
onload='renderMathInElement(document.body,{
  delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
  ]});'></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-2381283-3', 'auto');
	
	ga('send', 'pageview');
}
</script>

<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "httpdangkhoasdcgithubio" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

  
  <hr/>
  &copy; <a href="https://dkhoa.dev">Dang-Khoa</a> 2017 &ndash; 2019 | <a href="https://github.com/dangkhoasdc">Github</a> | <a href="https://twitter.com/dksdc">Twitter</a>
  
  </footer>
  </body>
</html>

