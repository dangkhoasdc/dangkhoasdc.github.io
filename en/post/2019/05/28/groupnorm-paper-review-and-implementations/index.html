<!DOCTYPE html>
<html
  lang="vi"
  prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"
>
  <head>
    <meta charset="utf-8" />

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="HandheldFriendly" content="True" />
<meta name="MobileOptimized" content="320" />
<meta name="viewport" content="width=device-width, initial-scale=1" />


<link rel="apple-touch-icon" sizes="180x180" href="http://dangkhoasdc.github.io//apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://dangkhoasdc.github.io//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://dangkhoasdc.github.io//favicon-16x16.png">
<link rel="manifest" href="http://dangkhoasdc.github.io//site.webmanifest">
<link rel="mask-icon" href="http://dangkhoasdc.github.io//safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">



<meta name="keywords" content="deep learning,
review,
">

<meta property="og:title" content="Groupnorm: Paper, review and implementations" />
<meta property="og:description" content="After several tries, I have found out that GroupNorm works suprisingly well on detection models. Just turn on the GroupNorm of FPN and I can get an improvement with a large margin. Going further, I want to replace Batchnorm of the backbone with GroupNorm and see how I can utilize this layer in other networks.
Paper: overview and comments Criticism about BatchNorm It does not work well on models trained with small batches." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://dangkhoasdc.github.io/en/post/2019/05/28/groupnorm-paper-review-and-implementations/" />
<meta property="article:published_time" content="2019-05-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-05-28T00:00:00+00:00" /><meta property="og:site_name" content="Dang-Khoa&#39;s blog" />

<meta property="og:site_name" content="Dang-Khoa&#39;s blog" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Groupnorm: Paper, review and implementations"/>
<meta name="twitter:description" content="After several tries, I have found out that GroupNorm works suprisingly well on detection models. Just turn on the GroupNorm of FPN and I can get an improvement with a large margin. Going further, I want to replace Batchnorm of the backbone with GroupNorm and see how I can utilize this layer in other networks.
Paper: overview and comments Criticism about BatchNorm It does not work well on models trained with small batches."/>


<meta itemprop="name" content="Groupnorm: Paper, review and implementations">
<meta itemprop="description" content="After several tries, I have found out that GroupNorm works suprisingly well on detection models. Just turn on the GroupNorm of FPN and I can get an improvement with a large margin. Going further, I want to replace Batchnorm of the backbone with GroupNorm and see how I can utilize this layer in other networks.
Paper: overview and comments Criticism about BatchNorm It does not work well on models trained with small batches.">
<meta itemprop="datePublished" content="2019-05-28T00:00:00+00:00" />
<meta itemprop="dateModified" content="2019-05-28T00:00:00+00:00" />
<meta itemprop="wordCount" content="803">



<meta itemprop="keywords" content="deep learning,review," />



    <title>Groupnorm: Paper, review and implementations || Dang-Khoa&#39;s blog</title>
    <link rel="canonical" href="http://dangkhoasdc.github.io/en/post/2019/05/28/groupnorm-paper-review-and-implementations/" />

    

    <link rel="stylesheet" href="/css/reboot.css" />
<link rel="stylesheet" href="/css/style.css" />
<link rel="stylesheet" href="/css/syntax.css" />



<script type="text/javascript" src="/js/main.js" defer></script>

<link href="https://fonts.googleapis.com/css?family=Lora&display=swap" rel="stylesheet">

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
  </head>
  <body
  class=" look-sheet-bkg"
  lang="vi">
  <div class="nav-bkg">
    <nav class="content-container pagewide-bar-padding">
      <span class="divider">/ </span>
      <a href="http://dangkhoasdc.github.io/" >Dang-Khoa&#39;s blog</a>
      <span class="divider">/ </span>
  <a href="http://dangkhoasdc.github.io/post">Posts</a>
      <ul class="list-unstyled right-links">

          <li>
            <a href="/en/post/">
              <span class="post-title">English version</span>
            </a>
          </li>

</ul>

    </nav>
  </div>
  <article
    id="main"
    class="content-container look-sheet article-pad-v "
    itemscope
    itemtype="https://schema.org/Article" >
  <meta itemprop="author" content="" />
  <meta itemprop="publisher" content="" />
  <meta itemprop="image" content="" />
  <h1 itemprop="name" id="title">Groupnorm: Paper, review and implementations</h1>
  <meta itemprop="headline" content="Groupnorm: Paper, review and implementations" />
  
  <div class="post-tags">
  
    <a href="http://dangkhoasdc.github.io/en/tags/deep-learning/">#deep learning</a>&nbsp;
  
    <a href="http://dangkhoasdc.github.io/en/tags/review/">#review</a>&nbsp;
  
  </div>
  
  
    
      <div class="post-date"><span itemprop="datePublished">May 28, 2019</span></div>
      <meta itemprop="dateModified" content="May 28, 2019"/>
    
  
  
  <div itemprop="articleBody" id="content" class="article-body margin-top-2em">
    <p>After several tries, I have found out that GroupNorm works suprisingly well
on detection models. Just turn on the GroupNorm of FPN and I can get an
improvement with a large margin. Going further, I want to replace Batchnorm
of the backbone with GroupNorm and see how I can utilize this layer in
other networks.</p>
<h1 id="paper-overview-and-comments">Paper: overview and comments</h1>
<h2 id="criticism-about-batchnorm">Criticism about BatchNorm</h2>
<p>It does not work well on models trained with small batches. Some papers showed that BatchNorm is mainly to get the activation distribution in control to help the training convergence. Therefore, if we already have good initialization, we don&rsquo;t even need to use BatchNorm.</p>
<h2 id="related-works">Related works</h2>
<ul>
<li>Local Response Normalization.</li>
<li>Batch Normalization (or Spatial Batch Norm in some frameworks).</li>
<li>Layer Normalization.</li>
<li>Weight Normalization.</li>
<li>Batch Renormalization.</li>
<li>Synchronized Batchnorm: [<a href="https://arxiv.org/abs/1902.04103">Bag of Freebies for Training Object Detection Neural Networks</a>] use this technique instead of <em>GroupNorm</em> for Yolov3, I wonder why.</li>
<li>Instance Normalization.</li>
</ul>
<h3 id="group-wise-computation">Group-wise computation</h3>
<ul>
<li>ResNext.</li>
<li>MobileNet. Note to self: after several weeks working on detection, we found out that MobileNet does not work well for detection. One paper supports this observation [<a href="https://arxiv.org/pdf/1905.10011.pdf">Light-Weight RetinaNet for Object Detection</a>] . Their observation is also consistent with my experiment results, especially about the low confidence scores of the model.</li>
<li>Xception.</li>
</ul>
<h2 id="normalization-revisiting-and-formulation">Normalization Revisiting and Formulation</h2>
<p>The authors did a nice job which is to unify the formula of popular normalization
techniques. Take a look the figure, in some extends, we can say that GroupNorm is a variant of LayerNorm and Instance Norm.</p>
<figure>
    <img src="/groupnorm/normalization_methods.jpg"
         alt="Normalization Methods"/> <figcaption>
            <p>Normalization Methods</p>
        </figcaption>
</figure>
<p>Interestingly, <em>Layer Norm</em> looks oddly like the pooling method in Triangulation Embedding, or other higher-level features. Based on the figure, we also figure out why <em>Batch Norm</em> is not effective on small batch size, namely N is small and there are not enough sample to compute the good approximation for two moments (mean and variance). So how do other methods try to overcome that problem? They try to compute the statistics on the channel itself. Most of the common CNN models use 64, 128 or 256 channels in the conv layer, hence we have relatively enough values in order to compensate the lack of samples in each batch.</p>
<p>Regarding the computation, the family of normalization layers are composed of two steps:</p>
<ol>
<li>Compute the statistics and normalize the input:</li>
</ol>
<p>$$ \hat{x_i} = \frac{1}{\sigma_i} (x_i - \mu_i) $$</p>
<p>where $\mu_i$ and $\sigma_i$ are computed for a subset $S_i$ of feature maps from a batch of input. The art of creating a new normalization layer is how to design a new subset which is able to overcome some shortcomings of previous methods.</p>
<ol start="2">
<li>For each channel, learn a linear transformation to compensate for the possible lost of representational ability:</li>
</ol>
<p>$$ y_i = \gamma \hat{x_i} + \beta $$</p>
<p>where $\gamma$ and $\beta$ are trainable scale and shift.</p>
<h1 id="implementation">Implementation</h1>
<p>The paper also mentioned the Tensorflow implementation, I don&rsquo;t want to talk about it much. However, <a href="https://github.com/pytorch/pytorch/blob/f4b434a6a53d9fe45283aee8572174f94a79f558/caffe2/operators/group_norm_op.h#L94">the C++ implementation</a> from caffe2 is worth reading. Why? Since it computes 2 moments in the inference time, hence I thought there is no way to integrate the layer into the penultimate Conv Layer, which is kind of disappointed since I want to optimize the inference model on mobile.</p>
<p>Interestingly, I&rsquo;ve also found that there are <a href="https://github.com/pytorch/pytorch/issues/1410">different implementations</a> for BatchNorm and there is no agreement across all popular deep learning frameworks about whether or not Bessel&rsquo;s corrections are applied.</p>
<p>Surprisingly enough, by using running standard deviations, we can avoid some serious numerical errors and get better approximation compared to the textbook formula of standard deviation. Some discussions are the problem and how to compute std can be found in The Art of Computer Programming, Volume 2, Section 4.2.2 or <a href="https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Example">wiki page of calculating variance</a>.</p>
<h1 id="experiments">Experiments</h1>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Label Recall</th>
<th>Label Precision</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset 1 (AffineChannel)</td>
<td>0.2464</td>
<td>0.3310</td>
</tr>
<tr>
<td>Dataset 1 (GroupNorm)</td>
<td>0.2676</td>
<td>0.3615</td>
</tr>
<tr>
<td>Dataset 2 (AffineChannel)</td>
<td>0.2492</td>
<td>0.3400</td>
</tr>
<tr>
<td>Dataset 2 (GroupNorm)</td>
<td>0.2620</td>
<td>0.3761</td>
</tr>
</tbody>
</table>
<p>From my own experiments whose model essentially are RetinaNet from Detectron library, GroupNorm indeed helps to improve the performance of detection models (+8% on recall and precision, of course, after setting the corresponding thresholds) on two COCOesque datasets. However, adapting GroupNorm to mobile frameworks may be difficult, some don&rsquo;t support this layer and we have to write our own CPU/CUDA code. Another workaround is to stick to BatchNorm, use smaller image size to train and increase the batch size. AffineChannel is another choice. It is relatively good and easily to merge into the conv layer in order to save the memory.</p>
<p>Nonetheless, GroupNorm is only used in the FPN layers in all settings. I wonder what happens if I replace all AffineChannel or Spatial BatchNorm by GroupNorm, even on the backbone. I will put the results soon (If I have time to do such experiments).</p>
<p>In conclusion, <em>GroupNorm</em> is simple yet effective normalization method to use in case you have to train models with small batch size.</p>

  </div>
</article>

  


  
    <div class="nav-bkg-50 content-container-narrow-pad bottom-links text-0p75">
      <nav class="flex-row">
      
      <a href="http://dangkhoasdc.github.io/en/post/2018/06/15/install-opencv-3-non-free-module-on-ubuntu/" class="flex-row v-center no-underline"  style="max-width:45%;">
        <span class="text-1p5">←</span>&nbsp;<span class="re-underline">Previous: Install OpenCV 3 non-free module on Ubuntu</span>
      </a>
      
      
        <a href="http://dangkhoasdc.github.io/en/post/2019/06/03/wip-libra-r-cnn-paper-review-and-comments/" class="flex-row v-center no-underline" style="max-width: 45%;">
        <span class="re-underline">Next: [WIP] Libra R-CNN: Paper review and comments</span>&nbsp;<span class="text-1p5">→</span>
        </a>
      
      </nav>
    </div>
  
  <footer>
    </footer>
    </body>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
onload='renderMathInElement(document.body,{
  delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
  ]});'></script>

  </html>
  

  </body>
</html>
