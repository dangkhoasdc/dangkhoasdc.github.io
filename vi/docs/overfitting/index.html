<!DOCTYPE html>
<html lang='vi'>

<head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='Đây là cuộc hành trình của mình tìm về bản chất của câu chuyện overfiting, underfitting và các khái niệm lung tung xung quanh như bias-variance tradeoff.
Tài liệu tham khảo chính được lấy từ 2 cuốn sách:
 Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol.'>
<meta name='theme-color' content='#ffcd00'>

<meta property='og:title' content='Overfitting là cái quái gì • Dang-Khoa'>
<meta property='og:description' content='Đây là cuộc hành trình của mình tìm về bản chất của câu chuyện overfiting, underfitting và các khái niệm lung tung xung quanh như bias-variance tradeoff.
Tài liệu tham khảo chính được lấy từ 2 cuốn sách:
 Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol.'>
<meta property='og:url' content='http://dangkhoasdc.github.io/vi/docs/overfitting/'>
<meta property='og:site_name' content='Dang-Khoa&#39;s blog '>
<meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/ff98eba7ca4592a175b13bd623f28f0c?s=256'><meta property='article:author' content='https://facebook.com/dangkhoasdc'><meta property='article:section' content='Docs'><meta property='article:tag' content='machine learning'><meta property='article:published_time' content='2018-03-29T00:00:00Z'/><meta property='article:modified_time' content='2018-03-29T00:00:00Z'/><meta name='twitter:card' content='summary'><meta name='twitter:creator' content='@dksdc'>

<meta name="generator" content="Hugo 0.53" />

  <title>Overfitting là cái quái gì • Dang-Khoa</title>
  <link rel='canonical' href='http://dangkhoasdc.github.io/vi/docs/overfitting/'>
  
  
  <link rel='icon' href='/favicon.ico'>
<link rel='stylesheet' href='/assets/css/main.809149b6.css'><style>
:root{--color-accent:#ffcd00;}
</style>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-2381283-3', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  

</head>


<body class='page type-docs'>

  <div class='site'>

    <a class='screen-reader-text' href='#content'>Skip to Content</a>

    <div class='main'>

      <nav id='main-menu' class='menu main-menu' aria-label='Main Menu'>
  <div class='container'>
    
    <ul><li class='item'>
        <a href='/vi/' class="">
          <span>Trang chủ</span>
        </a>
      </li><li class='item'>
        <a href='/vi/docs/' class="">
          <span>Docs</span>
        </a>
      </li><li class='item'>
        <a href='/vi/page/about/' class="">
          <span>dangkhoasdc</span>
        </a>
      </li><li class='item'>
        <a href='#posts' class="">
          <span>Posts</span>
        </a>
      </li><li class='item'>
        <a href='#about' class="">
          <span>Home</span>
        </a>
      </li><li class='item'>
        <a href='#projects' class="">
          <span>Projects</span>
        </a>
      </li><li class='item'>
        <a href='#publications_selected' class="">
          <span>Publications</span>
        </a>
      </li><li class='item'>
        <a href='/about/' class="">
          <span>About</span>
        </a>
      </li><li class='item'>
        <a href='/tags/notes/' class="">
          <span>Notes</span>
        </a>
      </li></ul>
  </div>
</nav>

      <header id='header' class='header site-header'>
        <div class='container sep-after'>
          <div class='header-info'><p class='site-title title'>Dang-Khoa&#39;s blog </p><p class='desc site-desc'></p>
          </div>
        </div>
      </header>

      <main id='content'>


<article lang='vi' class='entry'>
  <header class='header entry-header'>
  <div class='container sep-after'>
    <div class='header-info'>
      <h1 class='title'>Overfitting là cái quái gì</h1>
      

    </div>
    
<div class='entry-meta'>
  <span class='posted-on'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"/>
  <line x1="16" y1="2" x2="16" y2="6"/>
  <line x1="8" y1="2" x2="8" y2="6"/>
  <line x1="3" y1="10" x2="21" y2="10"/>
  
</svg>
<span class='screen-reader-text'>Posted on </span>
  <time class='entry-date' datetime='2018-03-29T00:00:00Z'>2018, Mar 29</time>
</span>

  <span class='byline'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M21,21V20c0-2.76-4-5-9-5s-9,2.24-9,5v1"/>
  <path d="M16,6.37A4,4,0,1,1,12.63,3,4,4,0,0,1,16,6.37Z"/>
  
</svg>
<span class='screen-reader-text'> by </span><a href='/vi/authors/khoa'>Dang-Khoa</a></span>
  

  
  
</div>


  </div>
</header>

  
  

  <div class='container entry-content'>
  <p>Đây là cuộc hành trình của mình tìm về bản chất của câu chuyện overfiting,
underfitting và các khái niệm lung tung xung quanh như bias-variance tradeoff.</p>

<p>Tài liệu tham khảo chính được lấy từ 2 cuốn sách:</p>

<ul>
<li>Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from
data. Vol. 4. New York, NY, USA:: AMLBook, 2012.</li>
<li>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of
statistical learning. Vol. 1. New York: Springer series in statistics, 2001.</li>
</ul>

<p>Nếu ai đó nói về hiện tượng overfitting, người nào học machine learning nghiêm
chỉnh sẽ nói overfitting là khi mô hình có low bias và high variance. Trong khi
đó thì underfitting là khi mà mô hình có high bias và low variance. Tuy nhiên,
2 khái niệm quái quỷ này xuất phát từ đâu.</p>

<p>Ta quay trở lại với câu chuyện về machine learning, mà theo bản thân mình, đây
chính là &ldquo;thống kê hiện đại&rdquo;. Nào bắt đầu.</p>

<p>Đặt bối cảnh là học có giám sát (supervised learning), ta có một tập dữ liệu
input và đồng thời thông tin được gán nhãn $(x, y)$. Trong đa số các thuật toán
học giám sát, bản chất của các mô hình chính là việc tìm ta hàm $f(x)$ sao cho
hàm này có thể xấp xỉ được y. Bởi ta không thể lấy toàn bộ mẫu của không gian
x, nên ta không thể xây dựng chính xác được $f(x)$, đó là lí do ta gọi $f(x)$ là
xấp xỉ.</p>

<p>Tuy nhiên, nếu giả sử vì 1 điều nhiệm màu nào đó, và với 1 mô hình huyền bí nào
đó, ta thực sự có được $F(x)$, tức với mọi $x$, $F(x)$ tính chính xác được $y$. Tuy
nhiên ta có 1 điều cần lưu ý, bởi sự gán nhãn, hay y, không phải luôn luôn
đúng, nên thay vì $y = F(x)$, ta có:</p>

<p>$$y = F(x) + eps$$</p>

<p>Với eps là nhiễu với mean = 0, variance = $\sigma^2$.</p>

<p>Và giờ đây là lúc điều kì diệu bắt đầu xuất hiện. Trong đa số các mô hình giám
sát, ta muốn tối thiểu độ lỗi của hàm xấp xỉ $f(x)$ và nhãn của dữ liệu. Một độ
lỗi phổ biến chính là MSE, ta muốn tối ưu kỳ vọng của MSE này (lý do dùng kỳ
vọng bởi ta tính MSE thông qua tập dữ liệu có trong tay):</p>

<p>$MSE = E[ (y - f(x)) ^ 2]$ , giờ là lúc trò mèo khai triển bắt đầu, để cho gọn
thì $f(x) = f$, và $F(x) = F$. Bởi $F$ xác định (bởi đây là hàm duy nhất và tính
chính xác quan hệ của x, y), nên $E[F] = F$. Đồng thời ta có $E[y] = E[F(x) + eps]
= E[F(x)] + E[eps] = F + 0 = F$.</p>

<p>Công thức quan trọng nhất trong các khai triển sau: $Var[A] = E[A^2] - E[A]^2$. (*)</p>

<p>Ta cũng có biểu thức sau:</p>

<p>$$ Var[y] = E[(y -  E[y])^2] = E[(F + eps - F)^2] = E[eps^2] = Var[eps] +E[eps]^2 = Var[eps] = \sigma^2 $$</p>

<p>$$ MSE = E[ y^2 - 2yf + f^2] = E[y^2] + E[f^2] - E[2yf] $$</p>

<p>$$ = Var[y] + E[y]^2 + Var[f] + E[f]^2 - 2yE[f]$$  (Công thức * và y deterministic).
$$= \sigma^2 + Var[f] + E[f]^2 - 2FE[f] + F^2$$ (bình tĩnh khai triển)
$$= \sigma^2 + Var[f] + (F - E[f])^2$$
$$= \sigma^2 + Var[f] + Bias^2$$</p>

<p>Tạm thời bỏ qua nhiễu của nhãn, ta có 2 hạng tử:</p>

<ul>
<li><p>Variance: độ biến thiên của mô hình đề xuất, hay trực quan hơn là độ  di chuyển
của mô hình đề xuất xoay quanh kỳ vọng của nó. Nếu ta có rất nhiều data, và cố
gắng fit toàn bộ data đó, một cách nào đó f của ta phải di chuyển rất nhiều
dẫn đến variance cao.</p></li>

<li><p>Bias: khoảng cách giữa kì vọng của mô hình đề xuất và mô hình
&ldquo;thật sự&rdquo;. Đây chính là chi phí của việc ta đã &ldquo;đơn giản hóa&rdquo; mô hình thực sự
F.</p></li>
</ul>

<p>Và giờ ta có thể thực sự hình dung một cách &ldquo;trực quan&rdquo; mô hình của mình thực
sự tốt thế nào.</p>

<p>Nếu ta cố gắng fit mọi điểm trong data đang có, ta sẽ khiến bias nó thực sự
thấp (bởi dùng rất điểm ra kq chính xác, nó sẽ rất gần với F &ldquo;thực sự&rdquo;), nhưng
đồng thời đây variance lên cao, hiện tượng overfitting xuất hiện. Điều ngược lại của sẽ
dễ thấy, nếu ta dùng quá ít data, f sẽ di chuyển ít (variance thấp), tuy nhiên
f ta ước tính sẽ rất xa F thực sự (bias cao).</p>

<p>Điều này giúp ta trực quan hơn về các mô hình học:</p>

<ul>
<li>Linear Regression: ta đã &ldquo;giả sử&rdquo; F thần bí là linear, 1 giả sử quá phiêu và
đơn giản, và do đó &ldquo;chi phí&rdquo; nó sẽ rất cao dẫn đến bias của linear regression sẽ
cao. Tuy nhiên, vì ta không cố fit chính xác toàn bộ data point dẫn đến variance
sẽ thấp.</li>
<li>KNN: kNN cố gắng fit toàn bộ data nó đang có (với k càng lớn) dẫn đến bias nó sẽ
về 0, nhưng càng lấy nhiều k, độ biến động của f càng cao dẫn đến variance cao.</li>
</ul>

<p>Biết được bản chất của bias và variance, ta có thể phân tích &ldquo;trực quan&rdquo; mô
hình đề xuất, qua đó có cái nhìn chính xác hơn với mô hình.</p>

</div>

  
<footer class='entry-footer'>
  <div class='container sep-before'>
  <div class='tags'>
  <span class='taxonomy-icon'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M20.59,13.41l-7.17,7.17a2,2,0,0,1-2.83,0L2,12V2H12l8.59,8.59A2,2,0,0,1,20.59,13.41Z"/>
  <line x1="7" y1="7" x2="7" y2="7"/>
  
</svg>
</span>
  <span class='screen-reader-text'>Tags: </span><a class='tag' href='/vi/tags/machine-learning'>machine learning</a></div>

  </div>
</footer>


</article>

<nav class='entry-nav'>
  <div class='container'><div class='prev-entry sep-before'>
      <a href='/vi/docs/opencv_android_studio_ndk/'>
        <span aria-hidden='true'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="20" y1="12" x2="4" y2="12"/>
  <polyline points="10 18 4 12 10 6"/>
  
</svg>
 Previous</span>
        <span class='screen-reader-text'>Previous post: </span>Cài đặt OpenCV trên Android Studio hỗ trợ lập trình NDK</a>
    </div><div class='next-entry sep-before'>
      <a href='/vi/docs/bioshock_infinite/'>
        <span class='screen-reader-text'>Next post: </span>Bioshock: Elizabeth hay học tăng cường [Reinforcement Learning]<span aria-hidden='true'>Next <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="4" y1="12" x2="20" y2="12"/>
  <polyline points="14 6 20 12 14 18"/>
  
</svg>
</span>
      </a>
    </div></div>
</nav>




      </main>

      <footer id='footer' class='footer'>
        <div class='container sep-before'><div class='copyright'>
  <p>&amp;copy; 2017 Dang-Khoa</p>
</div>

        </div>
      </footer>

    </div>
  </div><script>window.__public_path__='http:\/\/dangkhoasdc.github.io\/assets\/js\/'</script>

<script src='http://dangkhoasdc.github.io/assets/js/main.68cb493a.js'></script><script type='text/x-mathjax-config'>
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"processEscapes":true}})
</script>

<script type='text/javascript' async src='//unpkg.com/mathjax/MathJax.js?config=TeX-MML-AM_CHTML'></script>





</body>

</html>






















