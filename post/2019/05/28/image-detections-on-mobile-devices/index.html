<!DOCTYPE html>
<html lang="vi">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Image detections on mobile devices | My blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/books/">Books</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
      

    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Image detections on mobile devices</span></h1>

<h2 class="date">2019/05/28</h2>
</div>



<main>


<p>I have been working on image detection on mobile devices for the last 6 weeks,
which is not long enough to delve into some research paper on model compression
or pruning techniques but adequate to get into and conduct some experiments
on some detection models.</p>

<hr />

<p>Firstly, let look at the detection model and make some terms. A detection
usually (and from some of the state-of-the-art model: RetinaNet) is composed
of 3 components, from bottom to top:</p>

<ol>
<li>Backbone: feature extractions.</li>
<li>Region Proposal Module: produce proposals based on feature maps of the backbone
and feed them into the last component.</li>
<li>Head predictor: Predict the bounding boxes and label.</li>
</ol>

<p>In this note, I will talk about all 3 components when we develop an detection
model for mobile devices. Let assume that we are working on Android devices
and the deep learning frameworks on mobiles support some common layers, but
it is not identical to any other frameworks such as Tensorflow, Pytorch. Therefore,
we also have to deal with one more step, namely converting the model trained
from the workstation/servers into the model for mobile devices.</p>

<h1 id="model-capacity">Model Capacity</h1>

<h2 id="backbone">Backbone</h2>

<p>Here is the list of backbone I have been trying:</p>

<ul>
<li>Resnet50.</li>
<li>Resnet36.</li>
<li>MobileNetv1</li>
<li>MobileNetv2</li>
<li>SqueezeNet.</li>
</ul>

<h1 id="some-observations">Some observations</h1>

<h1 id="implementations">Implementations</h1>

<h2 id="converting-models">Converting models</h2>

<h1 id="conclusions-and-takeaways">Conclusions and Takeaways</h1>

<ol>
<li>If you have to deal with training dataset, make sure that the groundtruths,
annotations are consistent and correct because small networks do not have
large capacity to learn from label noise.</li>
<li>Lightweight networks behave very different between classification and
detection tasks. Make sure that you have proper experiments to justify the
model.</li>
<li>GroupNorm is good for detection, use it if it is available.</li>
<li>Sometimes, the FPN and head predictor are pretty heavy compared to the
backbone. Some tricks to reduce memory and improve the inference time are:

<ul>
<li>Remove certain octave scales, especially FPN 3.</li>
<li>Replace normal Conv layers in FPN branches with Depthwise layer.</li>
<li>Reduce anchor scales, aspect ratios or even number of classes if necessary.</li>
</ul></li>
<li>In general, Depthwise conv is a good choice to replace the normal conv layer.
However, the performance depends on the underlying implementations. You may not
gain any improvements because of it.</li>
<li>Different frameworks uses different settings, implementations even with
some de facto layers. While converting the model, make sure that the logic is
correct. You may have to retrain the model to the final settings based on
the mobile framework.</li>
</ol>

<h1 id="refernece">Refernece</h1>

<ul>
<li><a href="https://machinethink.net/blog/mobilenet-v2/">MobileNetv2</a>: Pretty good post about MobileNetv2. The author also mentions the implementation on iOS.</li>
</ul>

</main>

  <footer>
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
onload='renderMathInElement(document.body,{
  delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
  ]});'></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-2381283-3', 'auto');
	
	ga('send', 'pageview');
}
</script>

<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "httpdangkhoasdcgithubio" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

  
  <hr/>
  &copy; <a href="https://dkhoa.dev">Dang-Khoa</a> 2017 &ndash; 2019 | <a href="https://github.com/dangkhoasdc">Github</a> | <a href="https://twitter.com/dksdc">Twitter</a>
  
  </footer>
  </body>
</html>

