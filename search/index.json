[{"content":"","href":"/","title":"Home"},{"content":"","href":"/post/","title":"Documentation"},{"content":"","href":"/about-me/","title":"About Me"},{"content":" Why model? As I understood, using models is actually to apply mathematic into different problems in a unified framework.\nIntelligent Citizen of the world  Essentially, all models are wrong, but some are useful\n Model is the new \u0026ldquo;lingua franca\u0026rdquo;.\n Every one should know. All knowledge which human has known has models in it. It is everywhere, at any disciplines: economics (maximize payoff), biology (brain model, gene, species), political science, linguistics, law, game theory. Model helps explore knowledge from data, which makes us humble.  Clearer Thinker.  Make you become a better thinker. Allow us to inductively explore. Understand class of outcome:  Equilibrium. Cycle. Random. Complex.  Identify logical boundaries. Communicate ideas, data, observations.  Constructing models includes several steps:\n Name the parts: name all variables, factors, and constraints in the problem that are really matter, relevant to the problem. 2.  Understand data  Understand patterns. Structure information and transform into knowledge. Produce bounds. Retrodict. Predict other. Inform data collection. Estimate hidden parameters. Calibration.  Decide, Strategize, and Design  Decision aids: give us directions to give good decisions. Comparative statics. Counterfactual Identify and rank levers. Experimental design. Instituional design. Help choose among policies and institutions.  Courses Section Structure  The model  Assumptions Results Applications  Technical Details  Measures Proofs Practice Problems  Fertility  Definitions Types of model:\n Equation-based model. Agent based model includes:  Individuals: the object of the model. Associated behaviour Outcomes or Aggregation at the macro level.   Segregation and peer effects Sorting (homophily) and peer effects Peer Effect We choose to act like other people around us. For example, if we are hanging around with people who smoke, it is likely that over time, we also become smoker.\nSorting We choose to live, hangout with people with the same characteristics like us. For example, If we smoke, we want to like, or stay with people who smoke like us.\nThe group tends to have the same characteristics among its members.\nSchelling Tipping Model   Racial segregation in New York\n  1R 2B 3W 4B Rx 5R 6B 7R 8B  The agents deciding whether stay at the current location or move out bases on theirs neighbors who have the same characteristics. The rule is used a threshold to decide. For example, if there are more than 30% neighbors like me, I will stay.\nThis is a model of racial (or outcome) segregation.\nObservations: Even though people are tolerate, at the macro level we observe the segregation, segmentation in large-scale region. If people set their threshold especially high, that makes the whole region chaotic.\n Micromotives != macrobehaviour\n Exodus Tip: one agent moving causes others to move as well. Genesis Tip: some agents moving makes the agent move out.\n Index of dissimilarity  $$ {1 \\over 2} \\sum_{i=1}^{N} \\left| {a_i\\over A} - {b_i \\over B} \\right | $$\nLet $A$ and $B$ are the total of agents of demographics type 1 and type 2, respectively; $a_i$ and $b_i$ is the number of agents of each demographics in an area.\nGranovetter Model The collective behavior, e.g., social movement.\nStanding Ovation Model We change our behavior to match out with people around us.\nIdentification problem If we observe the segregation in data, the question is did they sort the behavior, or is this the peer effect?\n","href":"/post/thinking/why/","title":"Introduction to Model Thinking"},{"content":" The reasons why I want to learn this course:\n Deeply understand the material from my favorite fields such as mathematics, computer sciences, etc. Learn quickly and more effectively. Avoid procastination.  Focused and Diffuse Modes  Focused mode: concentration, when we focus on particular subjects, deeply lookinto the detail. The example from the lecture gives me a sense that this mode is the optimization methods being trapped at local region. Diffuse mode: give a broader look at the problem. This mode is used when we want to learn new materials. Different from the focused mode, this one is the random initialization, which helps us escape local minima.  Brain and Learning The brain weigh 3 pounds but consumes 10 times more energy by weight than the rest of the body. The brain is dynamic. After a sleep or a nap, the brain upgrades by generating more synapses on dendrites, the connection between neurons.\nFamous people use different think modes:\n Salvador Dali: dangling the key while sleeping while he was falling sleep. When the key droped and made noise, he wake up, the brain switch from the diffuse mode which connects different ideas to the focused mode. Thomas Edison: use the ball instead of the key. Fine, I will use my phone.  The point is to switch between two thinking modes back and forth.\nProcrastination When we really don\u0026rsquo;t want to something, the insular cortex is activated. This cortex is associated with pain. As a result, the brain switches the attention to something else. Procrastination resembles addiction:\n Temporary excitement. It is easy to fool yourself.    Construction of bad habits\n  Willpower consumes a lot of neural resources. That why we should use it wisely.\nChunking is related to habit. Habit is an energy saver which allows us to free our mind for other types of activities.\n The law of Serendipity: Lady Luck favors the one who tries\n Habit composed of 4 parts:\n The cue: the trigger launches \u0026lsquo;zombie\u0026rsquo; mode. Determine what triggers you into procrastination:  Location. Time Feeling. Reaction.  The routine: the habit mode, effortless mode in the brain to do an activity. Need to rewind the old habits.  Plan. Keep adjust the plan.  Daily \u0026ldquo;to-do\u0026rdquo; list. Weekly task list. Maintain leisure time. Eat your frog at the morning. *   The reward: The habit keeps continuing because it gives a pleasure after finish the routine.  Bigger rewards for bigger achivements.  The belief: The underlying belief reinforce the habit by repeatively claiming ourself that the habit is good.   Focus on process, not product\n Techniques  Pomodoro: set a timer to do a task, learn materials.  Set a short period time. No interruptions. Focus. Reward (web surfing, take a coffee, stretching, chatting)    Practice makes permanent\n Practice makes thought patterns impaint into the brain. Focusing intensely helps improve the formed patterns, strengthen the neuron structure.\nMemory There are multiple memory systems for different types of learning.\n  Long-term memory and short-term memory\n  Long-term memory  Distributed all areas of the brain. Can memorize a huge amount of memory. Take time and practice to move information from working memory to long-term memory:  Spaced repetition: distribute the practice session into many days, space the repetition out.  Located at hippocampus: this part of the brain is important for learning and memory of facts and events.  Sleep  Allows the brain to wash away metabolic toxins. Allows the brain to strengthen important parts of memories even as it arases less important memories. Allows the brain a chance to rehearse difficult material, going over and over the tougher aspects of what you are trying to learn. Makes a remarkable difference in your ability to figureout difficult problems and to understand what you are trying to learn.  Working memory (short-term memory)  Immediately and consciously processing. Located at preforntal cortex. Hold around 4-7 chunks of information. Keep repeating to remember chunks.  Techniques  Visual memory: easy to imagine, repeat and feedback. Use images and other senses to learn the concepts. Flash cards (Anki). Meaningful group: simplify the material. Memory Palace Technique: remember unrelated items. Use memorable images. Physical exercises.  Brain Model   Neuromodulators\n  Acetylcholine  Focus learning. Pay close attention. Leading to new long term memory.  Dopamine  Motivation. A part of the system controlling reward learning. Activated when we receive an unexpected reward. Is in the business of future rewards.  Serotonin  Strongly affects the social life. Risk-taking.  Chunking Definition   Context and chunks\n  A chunk is the neural network inside our brain firing together, which connects concrete ideas and information into a bigger pattern through meaning and use. It is later easily to access. A chunk is considered as a item of bottom-up learning mode. To improve understanding of a particular subject or material, a chunk needs to be associated with a context. From that, we can determine whether to use, access to that chunk based on the context. To gain mastery, we should use both the bottom-up learning (chunking) and top-down learning (a big picture) inside the context of knowledge.\n  The octopus of attention\n  The working memory is located at prefontal cortex which includes about 4 slots to access memory. When focusing, we has the \u0026ldquo;octopus of attention\u0026rdquo;. He use his tentacles slip through 4 slots to access memory. He losts his power when we are tired of, in pain, or any negative emotions.\nChunking is an activity in which we connect small pieces of information into a bigger. We can form a chunk by repetition, learning through examples, learning bit by bit from the material. We can arrange the methods into sequential steps:\n Focus on the material. Turn off all distractions. Understand the basic of materials. It glues all the connected information together. Learn by doing: solve the exercises, practice the material. Even if you understand something, it does not mean that you can naturally do it. Give context for the chunk. We dicide when to use the chunk or when not to use.  Techniques   Context and learning modes\n  One of the problem of learning and reviewing materials is the illusion of compentence. It means that by some ineffective learning methods such as re-reading, highlighting, we delude ourself into thinking that we have already understand the materials. It is also true with searching information on Internet, or practice one thing over and over again. To tackle this problem, several methods are proposed which can categorized into two groups: (1) learning methods (note-taking, text on margin instead of highlighting, chunking), (2) review methods (recall, mini-test delierate practice).\n Recall: instead of re-reading the material, recall help us remember information as well as gain better understanding. Mini-test: one of recall methods, frequently do mini-test for the subjects, mix things up between different subjects help us form the context of chunks. Deliberate practice: helps us gain mastery of the subject. If we keep doing the same problems over and over, the subject becomes easy, and thus we think that we truly understand the subject. Interleaving:  Readings  Try harder makes it more difficult to learn some aspects of language.  Adults excel at absorbing the vocabulary needed to naviate a grocery store or orderfood in a restaurant, but children have an ncanny ability to pick upon subtle nuances of language that often elude adults.   Renaissance Learning  Create a lively visual metaphor or analogy.  It is often helpful to pretend you are the concept you are tryingto understand.  Imposter syndrome. Memory-creativity tradeoff. Deliberate practice. Teamwork. Test checklist.   The first principle is that you must not fool yourself - and you are the easiest person to fool.\nInterviews  Terrence Sejnowski  Father of Boltzmann machine. Infomax algorithm for Independent Component Analysis  Some tidbits from the interview:\n Learn from experts. Learning by doing. Jogging and excercise help come up with new ideas, create new neurons. Passion and Persistent.  Nelson Dellis  Come up with pictures for all numbers. Memory Palace Technique. Omega 3.  ","href":"/post/learn_howto_learn/","title":"Learn How to Learn"},{"content":"Hello world . Processed by unicode2rstsubs.py, part of Docutils: . -- . Processed by unicode2rstsubs.py, part of Docutils: . -- This is a demo for pyweb\nFirst, we have to define the program, let call it hello.cpp:\nhello.cpp (1) =\n→header files for program (2) int main(int argc, const char* [] argv) { →print the message (3) }   ◊ hello.cpp (1).\n Header Files Basically, we only need a header file which handles the output of the message, namely std::cout:\nheader files for program (2) =\n#include \u0026lt;iostream\u0026gt; using namespace std;   ◊ header files for program (2). Used by: hello.cpp (1)\n  Messages To print the message Hello world, we simply put the output to the argument of std::cout:\nprint the message (3) =\ncout \u0026lt;\u0026lt; \u0026quot;Hello world\u0026quot; \u0026lt;\u0026lt; endl;   ◊ print the message (3). Used by: hello.cpp (1)\n It is done.\n ","href":"/post/a/","title":"Hello world with Literate Programming"},{"content":" The opencv_nonfree module provides very useful features such as two prominent image features SIFT and SURF (including the CUDA’s implementation). In this tutorial, I demonstrate how to install this module into OpenCV 3 on Ubuntu.\nPrerequisites  Remove the old version which has been installed in the system. Do not install opencv via apt-get Install cmake : sudo apt-get install cmake  Installation  Download opencv git clone https://github.com/opencv/opencv  Do not download the source from homepage.  Download the opencv_contrib : git clone https://github.com/opencv/opencv_contrib  Check both opencv and opencv_contrib are the latest versions otherwise we can not compile them.  Extract the source. cd path/to/opencv-source Run Cmake: cmake-gui In the textbox Where is the source code, it should be the path of OpenCV Where to build the binaries : path to your build directory, e.g, ~/opencv/build Select Configure When the dialog CMakeSetup pops up, configure the compiler according to your system. (Most of the time is to select Unix Makefiles as the generator and Use default native compilers as well.) After automatically generating Makefiles, CMake will show all available settings.  Select OPENCV\u0026gt;OPENCV_ENALBLE_NONFREE\n On OPENCV\u0026gt;OPENCV_EXTRA_MODULES_PATH , give the path to folder modules inside opencv_contrib\n Select Configure the project again. On the output window, check whether it produces Non-free algorithms are enabled .\n On BUILD , make sure that you have selected the appropriate modules for features extraction, e.g, BUILD_opencv_xfeatures2d.\n Configure the project again.\n Select Generate\n cd /path/to/opencv/build\n make all (To speed up : make all -j8 )\n make install\n  Done.\n","href":"/post/opencv3_nonfree/","title":"Install OpenCV 3 non-free module on Ubuntu"},{"content":" Transfer MAT objects from Android to NDK The main idea is to use the address of an MAT object in order to manipulate the data.\nBasically, we have a function playing as a bridge between Java APIs and NDK:\npublic native void function_name(long matAddress); To call the function, we use Mat\u0026rsquo;s address by calling getNativeObjAddr(). All computations in NDK will affect the content of MAT in both Java and NDK layers.\nIn the NDK code, to use cv::Mat object regarding java Mat, we can use static_cast:\nMat\u0026amp; im = *(static_cast\u0026lt;Mat*\u0026gt;(addrImg));  Notes There is a huge different in image channels between Java OpenCV and NDK OpenCV. When we decode the path or file to a bitmap in Java, we have to convert the bitmap to ARGB_8888 color channel, otherwise it does not work. Actually, it also can work on RGB_565 but for some reasons I can not remember, I always use ARGB_8888 in the project.\nBitmap bm32_image = bm.copy(Bitmap.Config.ARGB_8888, true); To manipulate the image correctly in NDK, we should covert it to the normal RGB channel, otherwise sometimes we get some bugs making us frustrating.\n// convert ARGB_8888 to RGB Mat im = new Mat(); Mat rgb_im = new Mat(); Utils.bitmapToMat(bm32_image, im); Imgproc.cvtcolor(im, rgb_im, Imgproc.COLOR_RGBA2RGB, 3); function_name(rgb_im.getNativeObjAddr()); Other practice is to put some assertions in NDK code to make sure that we the use the correct format for the input.\nOpenCV\u0026rsquo;s Camera OpenCV supports 3 types of camera:\n CameraBridgeViewBase. JavaCameraView. CameraSurfaceGLView.  JavaCameraView  Create the layout of the camera. For example, we can put the following lines to the Activity xml file:  \u0026lt;org.opencv.android.JavaCameraView android:layout_width=\u0026#34;fill_parent\u0026#34; android:layout_height=\u0026#34;fill_parent\u0026#34; android:id=\u0026#34;@+id/opencvcamera_view\u0026#34; \u0026gt;\u0026lt;/org.opencv.android.JavaCameraView\u0026gt;  Next, in the activity which controls the camera, we have to implement required methods from the CvCameraViewLisener2. There 3 methods are:   onCameraViewStarted. onCameraViewStopped. onCameraFrame.  Prior to processing the camera, we need to initialize the variable holding callbacks of three aforementioned methods:\nprivate CameraBridgeViewBase mOpenCVCamera; On the onCreate method:\nmOpenCVCamera = (CameraBridgeViewBase) findViewById(R.id.opencvcamera_view); mOpenCVCamera.setVisibility(CameraBridgeViewBase.VISIBLE); mOpenCVCamera.setCvCameraViewListener(this); Next, we implement 3 required methods:\n@Override public void onCameraViewStarted(int width, int height) { // initialize images, variables, settings  } @Override public void onCameraViewStopped() { // release the resources  } @Override public Mat onCameraFrame(CameraBridgeViewBase.CvCameraViewFrame inputFrame) { // retrieve the frame from `inputFrame`  // - the grayscale frame by imputFrame.gray()  // - the RGBA frame by inputFrame.rgba()  Mat im = inputFrame.rgba(); // do things  // postprocess: convert back to the RGBA image  return im; // `im` will show in the UI  } Noting that OpenCV\u0026rsquo;s Camera is not able to set the portrait mode. One workaround is to turn the Activity to landscape by putting the following line inside tag Activity in AndroidManifest.xml\nandroid:screenOrientation=\u0026#34;landscape\u0026#34; Data Manipulation unsigned char Mat It is troublesome when we want to assign a value of 255 to an unsigned char Mat because this language does not support unsigned char as a primitive type. One workaround is to allocate a 16S Mat, manipulate on that matrix, and finally convert to 8U.\nPoint2f and Point To convert MatOfPoint to MatOfPoint2f, we use the constructor:\nMatOfPoint matofpoint = new MatOfPoint(matofpoint2f.toArray()); Accessing the pixel values In order to retrieve and assign pixel value, we use the getter/setter from Mat.\nshort[] pixel = new short[nchannels]; m.get(i, j, pixel); // retrieve pixel values at (i, j) // in this example, the pixel has 3 channels pixel[0] = 255; pixel[1] = 0; pixel[2] = 125; m.set(i, j, pixel);","href":"/post/opencv_android_practices/","title":"OpenCV on Android: practices and tips"},{"content":" In this tutorial, I will demonstrate how to configurate the renowned computer vision libary, OpenCV, with the current Android Studio version (3.0.1). Let get started. The compiled version of OpenCV which supports Android is available at OpenCV Homepage. Download and extract it. Please note that to test the application properly on the mobile devices, the OpenCV Manager has to be installed.\nIn another tutorial, I will talk about how to compile our own OpenCV library and put it to Android Studio since the pre-compiled library misses some interesting and important components, e.g, the SIFT features and other licensed algorithms.\nCreating a new Android project with NDK support From the menu, select File-\u0026gt;New-\u0026gt;New Project and select Include C++ support in the Create New Project dialog.\n  Step 1   The next two steps are similar to the setup for normal applications, so I skip it. Later, the IDE asks the C++ settings for the project, I prefer C++14 over C++11 and also add two options, namely Exception Support and Runtime Type Information Support, for the project.\n From the Project Structure windows (File -\u0026gt; Project Structure), we add the OpenCV module by clicking on the plus sign, selecting the Import Eclipse ADT Project and pointing to the /path/to/OpenCV4Android/sdk/java directory.\n Select Modules -\u0026gt; app. In the Dependencies tab, add the OpenCV module by selecting the plus sign.\n Module Dependency. Select :openCVLibrary340.     Add openCVLibrary340   Configurate CMAKE and NDK In order to get CMAKE properly detect OpenCV, we add the following configuration to CMAKELists.txt:\n Add a new library:  include_directories(/path/to/OpenCV4Android/sdk/native/jni/include) add_library( lib_opencv SHARED IMPORTED ) set_target_properties(lib_opencv PROPERTIES IMPORTED_LOCATION /path/to/OpenCV4Android/sdk/native/libs/${ANDROID_ABI}/libopencv_java3.so)   Put lib_opencv to the arguments of target_link_libraries (the last command in CMakeLists.txt).  Beware that the path in CMAKE use / in both Windows and Linux.\nConfigurate Gradle  Put abiFilters 'x86', 'x86_64', 'armeabi', 'armeabi-v7a', 'arm64-v8a', 'mips', 'mips64' in the cmake setting.\n Put the following text inside android:\n  sourceSets { main { jni.srcDirs = ['src/main/cpp'] jniLibs.srcDirs = ['\\path\\to\\OpenCV-android-sdk\\\\sdk\\\\native\\\\libs'] } }  Build the project again and we are done. In the the couple tutorials I will demonstate several techniques to manipulate the libary on Android:\n Transfer OpenCV MAT from Android to NDK code. Organize the project and source code. -  ","href":"/post/opencv_android_studio_ndk/","title":"Setup OpenCV and Android Studio with NDK support"},{"content":"Programing Pearls - Jon Bentley\nExcellent.\nUntil now, there is no such book which makes me shout out like this after receiving from Amazon. Let brieftly review some favoriate books:\n CLRS: theoretically, this really is an \u0026ldquo;introduction\u0026rdquo; textbook about algorithms. Nonetheless, it still is an extensive reference and full of details. However, I am not impressed by the writing style and the pseudo-code.\n Algorithms, Sedgewick: one of the best textbooks ever. The code in the book is runnable, the analysis is interesting and rigorous (in contrast with CLRS uses Big-O, Sedgewick\u0026rsquo;s approach is nearly exact analysis). The method to prove the correctness and complexity is accessible as well as illustations in the book are fascinating. A drawback is that the paper is too thin so that it easily tears apart while note-taking.\n TAoCP: completely classical style and heavily mathematical analysis.\n Elements of Interview Programming: Cookbook for practicing algorithms. I do not have any special impressions of the book.\n  Programming Pearls is completely opposite to aforementioned books. This book is the perspective of an engineer on applying algorithms on work and solving other problems. More precisely, this is an adventure of Jon Bentley who is a renowned computer scientist known for kd-tree and Programming Pearls columns which are very famous during the period of 80s. [Don Knuth even send some literate programming code to Jon Bentley in order to introduce the concept].\nAlthough the book is quite thin compared to other \u0026ldquo;programming\u0026rdquo; or \u0026ldquo;algorithms\u0026rdquo; books, it possesses many \u0026ldquo;pearls\u0026rdquo; inside. The writing style is relaxing and engaging, intersperses the rigorous approach, extensive references and many intriguing puzzles which are worth giving a shot. What makes the book special is the way the author effiently uses scienctific approach to solve industrial problems as well as the viewpoint of an engineer about algorithms.\nPerhaps Steve McConnell already gave the best description about this book:\n \u0026hellip; Programming Pearls was one of the most influential books I read early in my career, and many of the insights I first encountered in that book stayed with me long after I read it. \u0026hellip;\n ","href":"/post/programming_pearls/","title":"Programming Pearls"},{"content":" The content of this note mostly belongs to the first section of TAoCP, Vol4A.\nDefinition Given a square matrix of size $M$ and each element is $\\{0, 1, \\cdots, M-1 \\}$, we construct a matrix such that for the element $i$ in the set $\\{0, 1, \\cdots, M-1\\}$ only appears exactly 1 time on every row and column.\nExamples We have 16 cards which comprises a combination of 4 ranks, i.e, J, K, Q, A and 4 suits: $\\heartsuit, \\diamondsuit, \\clubsuit, \\spadesuit$, we arrange the cards such that for each column or row, we have 4 ranks and 4 suitst (Jacques Ozaman, mathematiques et physiques, Paris 1725).\n  One instance of such the arrangment. Source: TAoCP, Vol 4A.   Applications While studying mathematics, sometimes we could not find an obvious application in order to motivate ourself to study the concept. However, since am a computer science engineer who is pragmatic than _mathematical inclination, I am specially interested in concepts which have applications in real life. Regards the latin square, we get several ones:\n Sudoku: the variant of latin square is one of the most popular puzzles in the world. Design of experiments. Error corecting codes. Crytography.  War Story In 1779, a puzzle similar to aforemention example attracted the famous mathematician: Leonhard Euler:\n Thirty-six officers of six different ranks, taken from six different regiments, want to march in a $6 \\times 6$ formation so that each row and each column will contain one officer of each rank and one of each regiment. How can they do it?\n Even though being amost blind, Euler was determined to solve the puzzle and tried to generalize the problem for other cases $M = 1, 2, \\cdots$. However, when $M\\mod 4 = 2$, he could not solve it. Later, he stated a conjenture that there is no solution for $M \\mod 4 = 2$.\nHowever, the puzzle Euler tackled is quite different compared to the definition given in the previous section. More precisely, let $\\heartsuit = 1, \\diamondsuit = 2, \\clubsuit = 3, \\spadesuit = 4$, and we divide ranks and suits into 2 seperated matrices, we got two Latin squares:\n$$ \\begin{bmatrix} J\u0026amp; A\u0026amp; K\u0026amp; Q\\newline Q\u0026amp; K\u0026amp; A\u0026amp; J\\newline A\u0026amp; J\u0026amp; Q\u0026amp; K\\newline K\u0026amp; Q\u0026amp; J\u0026amp; A \\end{bmatrix} $$\nand\n$$ \\begin{bmatrix} 2\u0026amp; 1\u0026amp; 4\u0026amp; 3\\newline 4\u0026amp; 3\u0026amp; 2\u0026amp; 1\\newline 3\u0026amp; 4\u0026amp; 1\u0026amp; 2\\newline 1\u0026amp; 2\u0026amp; 3\u0026amp; 4 \\end{bmatrix} $$\nFig. 1 becomes:\n$$ \\begin{bmatrix} J2\u0026amp; A1\u0026amp; K4\u0026amp; Q3\\newline Q4\u0026amp; K3\u0026amp; A2\u0026amp; J1\\newline A3\u0026amp; J4\u0026amp; Q1\u0026amp; K2\\newline K1\u0026amp; Q2\u0026amp; J3\u0026amp; A4 \\end{bmatrix} $$\nTwo Latin sqaures is orthogonal iff when we combine two elements with the same position together, each element in the new matrix is unique. The puzzle of Euler is: given a Latin sqaure, how do we find another Latin matrix which is orthogonal to the given one.\nThe combined matrix is called as a Graeco-Latinx matrix.\nIn order to prove Euler\u0026rsquo;s conjenture, mathematics spent almost 180 years with the support from computers. In 1957, Paige and Tompkins programmed on SWAC to find a counterexample with $M=10$. After 10 hours running without any results, they terminated the program.\nIn 1960, Parker et al. proved that it is possible to find an orthogonal Latin square when $M \\geq 6$ and disprove the conjenture. In addition, they also wrote a program on UNIVAC 1206 which is able to generate the matrix. The program only took nearly 1 hour to find a solution of input in 1957:\n0 1 2 3 4 5 6 7 8 9 1 8 3 2 5 4 7 6 9 0 2 9 5 6 3 0 8 4 7 1 3 7 0 9 8 6 1 5 2 4 4 6 7 5 2 9 0 8 1 3 5 0 9 4 7 8 3 1 6 2 6 5 4 7 1 3 2 9 0 8 7 4 1 8 0 2 9 3 5 6 8 3 6 0 9 1 5 2 4 7 9 2 8 1 6 7 4 0 3 5  One instant of the solution (TAoCP):\n0 2 8 5 9 4 7 3 6 1 1 7 4 9 3 6 5 0 2 8 2 5 6 4 8 7 0 1 9 3 3 6 9 0 4 5 8 2 1 7 4 8 1 7 5 3 6 9 0 2 5 1 7 8 0 2 9 4 3 6 6 9 0 2 7 1 3 8 4 5 7 3 5 1 2 0 4 6 8 9 8 0 2 3 6 9 1 7 5 4 9 4 3 6 1 8 2 5 7 0  Transversals Parker utilized a concept called transversal which was previously proposed by Euler. A transversal is a way to choose a sequence of $M$ elements such that there is only 1 element in each row, each column and is unique in the sequence.\nFor example: 0859734216, it means that we choose 0 in the first column , 8 in the second column 2, 5 in the third one, etc. Supposed that $k$ is the first item of a transversal, based on following elements, we replace them by $k$. Given $M$ transversals of $k = 0, 1, \\cdots, M-1$, we are able to construct an orthogonal matrix. For instant, with the sequence 0859734216, we place 0 at the 0th row of the first column, the 8th row of the second column, and so on.\nThe algorithm below counts the number of transversals of a Latin square.\ntypedef vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; mat; classtransversal { private: int n; vector\u0026lt;bool\u0026gt; row; vector\u0026lt;bool\u0026gt; nums; mat input; int cnt; public: transversal(const mat\u0026amp; in_): n(in_.size()), input(in_), row(n, false), nums(n, false), cnt(0) { assert(in_.size() == in_[0].size()); } void print() { // debugging  for (auto\u0026amp; rows: input) { for (auto\u0026amp; e: rows) cout \u0026lt;\u0026lt; e \u0026lt;\u0026lt; \u0026#34; \u0026#34;; cout \u0026lt;\u0026lt; endl; } } int compute(int idx) { cnt = 0; fill(row.begin(), row.end(), false); fill(nums.begin(), nums.end(), false); nums[input[idx][0]] = true; row[idx] = true; comp(1); return cnt; } private: void comp(int idx) { //idx = index of column  if (idx == n) { cnt++; return; } for (int i = 0; i \u0026lt; n; ++i) { if (!row[i] \u0026amp;\u0026amp; !nums[input[i][idx]] ) { row[i] = true; nums[input[i][idx]] = true; comp(idx+1); row[i] = false; nums[input[i][idx]] = false; } } } };  From the above input, we get the output:\n79 96 76 87 70 84 83 75 95 63  It is exactly the same as in the book. We later can modify the code to find all transversals given an input. After that, for every transversal $k$, we choose a set such that there is no duplicate elements which becomes the output matrix. The details of the algorithm will be discussed in later posts about Dancing Links.\nReferences  Knuth, Donald E. The Art of Computer Programming, Volume 4A: Combinatorial Algorithms, Part 1. Pearson Education India, 2011. https://en.wikipedia.org/wiki/Latin_square http://elscorcho.50webs.com/latinsquares.pdf  ","href":"/post/latin_square/","title":"Latin squares and a story of computer science history"},{"content":" [It is such a long time since I wrote a dedicated technical post about computer vision.]\nIntroduction Since Computer Vision is an interdisciplinary research which involves different fields such as physics, biology, computer science, neuroscience, etc. For this reason, there are different approaches when people consider about the history of computer science. In this post, I summarize two viewpoints of Prof. Fei-Fei Li, and Prof. Richard Szeliski from their courses [1] and textbooks [1]. In the early period of the research, computer vision is considered as a subfield of Artificial Intelligence. In the 1950s, the major goal of Artificial Intelligence is to create a framework which is able to simulate the brain mechanism of humans. As regards Computer Vision, majority research problems at that time is also to find a system which is able to understand and manipulate visual inputs as the human cognition system.\nWhy and how can visual systems precisely observe and interpret a scane which is received from eyes? This question requires us study the machenism of visual system from organisms and the development of the system throughtout evolution. There are many prominent discoveries in many fields engaging when scienctists were trying to find the answer.\n543M BC   The Cambrian explosion, source: Papermasters   This is the Big Bang of evolution since prior to this particular time, there are few origanims existing in the planet. And suddenly, a huge number of living beings appears on Eather. Based on archaeological evidences, there are several hypothesises are proposed in order to explain the phenomenon. Perhaps a meteor brought those livings, or perhaps there were tremendous changes in weather, enviroments which transform the whole biology on the planet. [These assumptions are extensively discussed in Cosmos, Carl Sagan].\nBiologists called the phenomenon as the Cambrian explosion, in a short period when most major animal appeared in the fossil record. One assumption to reason the phenomenon is that there emergence of visual system on animals. At the first time in history, organisms no longer were susceptible to surroundings since the visual system help them flexibly interact with environemnts ad well as avoid the predators. Thank to these systems, animals are able to survive, navigate, and manipulate. These activities later are considered as the most important problems in artificial visual system, i.e, to create a system that can survive, navigate, and manipulate surrounds1.\nwhich I will talk about in another post. discussion this\n16th Century   Camera Obscura and a painter. Nguồn: unknown.   Throughout history, mandkind progressively invented devices which are able to capture a scene. There are many inventions around the world, e.g, in China, Egyt and other ancient civilizations. However, util the 16th century, Leonardo da Vinci is the first man who obtains rigorous records which discuss how these devices work in detail. Later, these mystery devices are called \u0026ldquo;cameras\u0026rdquo;.\n  Ghi chép của Leonardo da Vince về mắt người.   In his documents, Leonardo used the pinhole principal to model the machenic by which a camera can capture an scene. Sometimes, it is also referred as camera obscura. In addition, he applied this principal in order to understand how human eyes function. [This book describes extensively Leonardo\u0026rsquo;s incredible works.]\nIn this particular period, there are many inventions of the camera. Nowadays, cameras beocme one of the most popular device around the world. As in 2016, the number of visual sensors are more than the world population. However, these inventions as well as Leonardo\u0026rsquo;s discovers just end up copying the visual environment and cameras still could not understand neither interpret what they received.\nIn the next couple of posts, I will discuss the progress of computer vision in the modern era, especially 1950-1970 when Artificial Intelligence was sheer active in research community.\n It is quite similiar to another area of machine learning: Reinforcement Learning, [return]   ","href":"/post/compvision_hist/","title":"History of Computer Vision - Part 1"},{"content":" The programming exercise is from TAoCP, Vol3, 5.1.1-6:\n Design an algorithm that computes the inversion table $b_1, b_2 \\cdots b_n$ corresponding to a given permutation $a_1a_2 \\cdots a_n$ of ${1, 2, \\cdots , n}$, where the running time is essentially proportional to $n\\ log n$ on typical computers.\n I was really stuck on the solution Knuth given in the book. The author also mentioned another approach which actually is a modifination of merge sort. But let first understand the algorithm using bitwise.\nImplementation The C++ implementation has a bit different from the original one. I used 0-index array instead of 1-index array as Knuth\u0026rsquo;s version. Frankly, it is not the best version, I just want to convert the pseudo code to an executable one.\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt; using namespace std; int main(int argc, char const* argv[]) { // input  int n; cin \u0026gt;\u0026gt; n; vector\u0026lt;int\u0026gt; v(n, 0); for (auto\u0026amp; vi: v) cin \u0026gt;\u0026gt; vi; // algorithm: TAoCP vol 3. 5.1.1-6  // init  vector\u0026lt;int\u0026gt; b(n, 0); vector\u0026lt;int\u0026gt; x((n\u0026gt;\u0026gt;1)+1, 0); int k = 0, r, s; for (int nn=n; nn\u0026gt;1; nn \u0026gt;\u0026gt;= 1) k++; // compute floor(lg(n))  for (; k \u0026gt;= 0; k--) { // traversal through bits 0 -\u0026gt; \\floor(\\lg N)  for (int s = 0; s \u0026lt;= n\u0026gt;\u0026gt;(k+1); s++) // init array x = 0 \\forall elements  x[s] = 0; for (int j = 0; j \u0026lt; n; ++j) { r = (v[j] \u0026gt;\u0026gt; k) \u0026amp; 1; s = v[j] \u0026gt;\u0026gt; (k+1); if (r) x[s] += 1; else b[v[j]-1] += x[s]; } } // output  for (auto bi: b) cout \u0026lt;\u0026lt; bi \u0026lt;\u0026lt; \u0026#34; \u0026#34;; cout \u0026lt;\u0026lt; endl; return 0; }  Analysis About the running time, it is discernible since the outer loop of $k$ costs $\\lfloor\\lg N\\rfloor$ and two inner cost $k+2$ and $N$, respectively.\nBut how on earth does the algorithm work? It is too subtle to be understand at the first time we saw the solution.\nGiven the index of bitstring $k$, we consider 2 strings having the forms $\\overline{s1T}$ and $\\overline{s0T}$. Given a fixed $\\overline{s}$, $T$ and any $T\u0026rsquo;$, we always know that $\\overline{s1T\u0026rsquo;}$ \u0026gt; $\\overline{s0T}$. x[] plays a role as a counter which checks how many elements of $\\overline{s1T\u0026rsquo;}$ we have browsed and the inversion $b[\\overline{s0T}]$ updates its value by $x[\\overline{s}]$.\nFor example, let input be $5, 9, 1, 8, 2, 6, 4, 7, 3$, we have the following binary codes:\n5: 0101 9: 1001 1: 0001 8: 1000 2: 0010 6: 0110 4: 0100 7: 0111 3: 0011  Let run the algorithm step by step:\n k=3. $\\overline{s} = \\overline{0}$, x[0] = 2. b = 1 2 2 2 0 2 2 0 0. k=2. Array x have two items $x[\\overline{0}]$, and $x[\\overline{1}]$ whose values are 4, 0, respectively. b = 2 3 6 2 0 2 2 0 0. k=1. There are 3 posibilities of $x[s]$, namely $x[\\overline{00}]$, $x[\\overline{01}]$, $x[\\overline{10}]$ whose values are 2, 2, 0, respectively. Eventually, b is 2 3 6 3 0 2 2 0 0. k=0, There are 5 items in x: $x[\\overline{000}] = 1$, $x[\\overline{001}]=1$, $x[\\overline{010}]=1$ ,$x[\\overline{011}]=1$, $x[\\overline{100}]=1$, we get the final output b = 2 3 6 4 0 2 2 1 0.  The mergesort-based algorithm Instead of constructing an inversion table, this algorithm count the total number of inversions in a permutation which utilize merging procedures from the renowned mergesort.\nclassinversion { private: vector\u0026lt;int\u0026gt; x, aux; long long int cnt; public: inversion(const vector\u0026lt;int\u0026gt;\u0026amp; x_): x(x_), aux(x_.size()), cnt(0) { merge_sort(x, 0, x.size()); } //ctor  void merge_sort(vector\u0026lt;int\u0026gt;\u0026amp; a, int low, int high) { if (low \u0026gt;= high-1) return ; int mid = low + (high-low)/2; merge_sort(a, low, mid); merge_sort(a, mid, high); merge(a, low, mid, high); } void merge(vector\u0026lt;int\u0026gt;\u0026amp; a, int low, int mid, int high) { int subidx_1 = low; int subidx_2 = mid; for (int j = low; j \u0026lt; high; ++j) { if (subidx_1 \u0026gt;= subidx_2) aux[j] = a[subidx_2++]; else if (subidx_2 \u0026gt;= high) aux[j] = a[subidx_1++]; else if (a[subidx_1] \u0026lt;= a[subidx_2]) aux[j] = a[subidx_1++]; else { cnt += (mid-subidx_1); // (1)  aux[j] = a[subidx_2++]; } } // copy back to a  copy(aux.begin()+low, aux.begin()+high, a.begin()+low); } inline long long int count() const { return cnt;} };  This is the exercise 5.2.5-21 in TAoCP. Unfortunately, Knuth did not mention the solution. The only modification is to add (1) to the merging step which counts the number of inversions of a[subidx_2], the item in the second array we consider while merging two arrays. Since from mid to subidx_2, all items are lesser or equals to a[subidx_2], so there is 0 inversions. However, since a[subidx_1] \u0026gt; a[subidx_2] it means that all items from subidx_1 to mid are greater than a[subidx_2] given the invariant that two arrays are already sorted. Therefore, there are mid-subidx_1 inversions of a[subidx_2].\nReference  gywangmtl\u0026rsquo;s post: the post trully helps me understand the algorithm. Also, thank you, Google Translates.  ","href":"/post/gen_inversion_table/","title":"Generate the inversion table from an integer sequence"},{"content":" Book Info:\n    Description     Title Selected Papers on Computer Science [Amazon]   Author Donald Knuth   Pages 276    This is the most accessible book from Don Knuth. Although I was published nearly 20 years ago, it still is a classic computer science book. In Amazon, there is an interesting comment about the book from Peter Norvig (Director of Google Research). The major topic in the book is the origination of computer science in the period of which the discipline is a new thing. Through chapters, we can see there were many debates and struggles among scientists about whether if computer science is truely a \u0026ldquo;science\u0026rdquo; and not a branch of mathematics [Chapter 1, 2, 3].\nThis makes me think about the current state of Deep Learning. Maybe 5 or 10 years later, Deep Learning will become a separate discipline as Computer Science segragated from mathematics several decades ago.\nThere are especially interesting chapters in the book which I can describe as below:\n Chapter 0: a general overview about Computer Science. Chapter 1: Computer Science and its relaton to mathematics: the difference between modern mathematics and computer science. Besides, the author also mentioned the analysis of the classic algorithm: hashing. Chapter 2 and 3: The overview about algorithms as well as the approach by which the author solves algorithmic problems. Although the chapter is published long time ago (1976-77) which maybe quite new at that time, in my opinion these mentioned algorithm, namely the shortest paths, searching and combinatoric optimizations, become classic research in CS nowadays. Chapter 6-9: Theory and Practice: the whole book is distilled into these chapters. Historic text actively disscuss the most important aspects in CS. Chapter 11-13: The history of Computer Science: From ancient civilization uses algorithms to solve practice problems to the very first analysis of John von Neumann about merge sort.  History of Computer Science and Deep Learning While reading the book, I had the feeling that the period of 1950-1975 perhaps exploded into computer science research, which is pretty much similar to Deep Learning nowadays. That was the time we there are few universities opened Computer Science department and people still debated about the name of this science that whether its name is \u0026ldquo;Computer Science\u0026rdquo;, \u0026ldquo;Information Technology\u0026rdquo;, or \u0026ldquo;Information Processing\u0026rdquo;. That is also the time when people thought that the problem which can only solve on $\\mathcal{0}(N^2)$ actually can be solved in $\\matchcal{O}(N \\lg N)$, the time when there is not any mathematical tools for analyzing algorithms.\nHow about Deep Learning?\nNowadays, people remain skeptical about Deep Learning in many aspects. Somebody compared it to alchemy because of the lack of rigorous analysis and mathematical fundamentals. However, according to the currect development of Deep Learning and avoiding the media hype, I am confidient about the future of Deep Learning. At least, in my opinion, it will become an interdiscipline and completely transform into \u0026ldquo;chemistry\u0026rdquo;. In retrospect, perhaps Computer Science was compared to \u0026ldquo;alchemy\u0026rdquo; in the period of 1940-50 due to its lack of rigor.\nStudy about History of Computer Science I was fascinated by the analysises in detail about \u0026ldquo;ancient\u0026rdquo; algorithms as well as the elaborate way to discuss the draft of merge sort from John von Neumann. IMO, this really is the way people should study about the history of Computer Science. It is not about memorizing who and when algorithms were created, it is about the motivation and approach which the inventor tackled the problem as well as analyzing the methods in the circumstances which the technology is limited. Through these insights, we will respect the contribution from algorithms and their authors. In addition, we can gain more methods, approached for another problems.\nDon Knuth is especially interested in this particular subject, there are many videos and documents in which he discussed exhaustively:\n Let\u0026rsquo;s Not Dumb Down the History of Computer Science: it is worth every minute watching. I love this comment from Youtube: \u0026ldquo;Discover how discoveries are discovered\u0026rdquo;. History of pattern generation algorithms: [fasc4b] Hamiltonian path: [fasc8a].  ","href":"/post/selectedpapers_cs/","title":"Selected Papers on Computer Science (Don Knuth) and the current state of Deep Learning"},{"content":" The repertoire method is an method of finding closed-form of recurrence relations and sum of a series. The method is introduced in Chapter 1 of ConMath but most readers at the first time seem to struggle with it. Through the book, especially Chapter 1 and 2, the repertoire method demonstrates its ability to solve many sums and recurrences. However, I honestly admit that it is quite difficult to apply and fully understand how it works. In this note, I try to figure the way to effectively apply the method for solving recurrences.\nDefinition In essence, the main goal of the method is to find coefficients for a linear combination of a set of recurrences. This method works very well on linear recurrences, in the sense that the solutions can be expressed as a sum of coefficients multiplied by functions of $n$ Therefore, if we have to find a closed-form of a linear recurrence, this is worth trying.\nThe method is not described clearly in ConMath because the authors always come up with a set of recurrences and get their coefficients but they do not mention how they figure out. Regarding this aspect, Sedgewick\u0026rsquo;s book obtains accessible clarification and systematic examples. Therefore, I use the procedure in Sedgewick\u0026rsquo;s book as a recipe for the repertoire method:\n Relax the recurrence by adding an extra function term. Substitue known functions into the recurrence to derive identities similar to the recurrence. Take linear combination of such identities to derive an equation identical to the recurrence.  At the first look, it is quite abstract. However, some examples carried out in the book illustrate how we manipulate the steps. Suppose that we have recurrence $a_{n} = a_{n-1} + {\\text stuff} $. First we generalize the identity by replacing ${\\text stuff}$ with $f(n)$, i.e., $a_{n} = a_{n-1} + f(n)$. We easily obtain $f(n) = a_{n} - a_{n-1}$. The next step is to build a table of ingredients in which we can construct $f(n)$. Finally, we determine coefficients of each component so that they satisfy $f(n)$ and the basis of the recurrence.\nFor more details and explanations, I strongly recommend reading Markus Scheuer\u0026rsquo;s answer. Of course, ConMath (Chapter 1, 2, 6) is a good material for practicing the method except understanding purposes. Section 2.5 from Sedgewick\u0026rsquo;s book summarizes some approaches to finding the closed-form including the repertoire method. Examples in the book are required reading.\nThe best way to fully understand the method is to work through examples.\nExamples Closed-form of sums We can easily convert sum of a sequence into a linear recurrence. Let begin with the first example which only contains term $n^3$.\n$ S_{n} = \\sum_{k=0}^n k^3$\nThis sum can be seen as:\n$ a_{0} = 0$ $ a_{n} = a_{n-1} + n^3$\nNext, we build a table of $a_{n}$ and $a_n - a_{n-1}$.\n   $a_n$ $a_n -a_{n-1}$     1 0   n 1   $n^2$ $2n-1$   $n^3$ $3n^2-3n+1$   $n^4$ $4n^3-6n^2+4n-1$    How can I fill $a_n$? Since we want $f(n)$ obtains $n^3$ and we observe that $f(n) = a_n - a_{n-1}$ depends on the degree of $n$ in $a_n$ and hence we come up with some simple sequence, i.e., computing $a_n - a_{n-1}$. Turn out $f(n)$ have smaller degree of $n$ than $a_n$ exactly 1. $f(n)$ obtaining $n^3$ means that $a_n$ has to obtain $n^4$. The simplest form we can come up with is $n^3 = \\alpha (4n^3-6n^2+4n-1)$. So $\\alpha = {1\\over 4}$, we need to eliminate $n^2$, $n$ and constants. In the second attempt, we add $n^3$ and $n^2$ into the linear combination:\n$$ \\alpha (4n^3-6n^2+4n-1) + \\beta(3n^2-3n+1) + \\lambda (2n-1) = n^3 $$\n$$ \\begin{cases} -6\\alpha + 3\\beta = 0\\newline 4\\alpha -3\\beta + 2\\lambda = 0\\newline -\\alpha + \\beta - \\lambda = 0 \\end{cases} $$\nThe solution is $(\\alpha, \\beta,\\lambda) =({1\\over 4}, {1\\over 2}, {1\\over 4}) $ and hence $a_n = \\alpha n^3 + \\beta n^2 + \\lambda n = {1\\over 4}n^2(n+1)^2$. Also, since $a_0 = 0$, it is the final solution.\nLet try another example, this time we use an exercise in ConMath:\n$$ S_{n} = \\sum_{k=0}^{k} (-1)^kk^2$$\n   $a_n$ $a_n-a_{n-1}$     $(-1)^nn^2$ $(-1)^n(2n^2-2n+1)$   $(-1)^n $ $2(-1)^n$   $(-1)^nn $ $(-1)^n(2n-1)$    The solution is $S_n = {1\\over 2}(-1)^n n(n+1)$ since we just add the first term and the last term together and then divide by 2, we get $f(n)$. When I first saw the sum, I could not think any sequences which help me find $f(n)$. After play which some sequences, I realize that assigning $a_n = f(n) = (-1)^nn^2$ actually lets other patterns be discovered.\nLinear Recurrences $ a_0 = 7 $\n$ a_n = a_{n-1} + 2n^2 + 7 \\; n \u0026gt; 0 $\nBased on the table we built in previous examples, we can construct a linear combination for the recurrence:\n$\\alpha(3n^2-3n+1) + \\beta(2n-1) + \\lambda 1 = 2n^2+7$\n$$ \\begin{cases} 3\\alpha = 2 \\newline -3\\alpha + 2\\beta = 0 \\newline \\alpha - \\beta + \\lambda = 7 \\end{cases} $$\nThe solution is $(\\alpha, \\beta, \\lambda) = ({2\\over 3}, 1, {22\\over 3})$. However, at this time $a_0= 0 \\neq 7$, we have to add constant $7$ to the final form. The closed-form is $a_n = {2\\over 3}n^3+n^2+{22\\over 3}n+7$.\nExercise 1.16 (ConMath) $$ g(1) = \\alpha $$ $$ g(2n+j) = 3g(n) + \\gamma n + \\beta \\; j = 0, 1 ; n \\geq 1 $$\nRelated Materials After hours searching on Internet, I found some useful links which actually helps me understand the method:\n Wakatta\u0026rsquo;s post Math stackexchange answer: this one is damn good. Pindancing\u0026rsquo;s post Sedgewick, Robert, and Philippe Flajolet. An introduction to the analysis of algorithms. Pearson Education India, 1996. [IMO, the explanation in this book is much better than in ConMath.] Graham, Ronald L., et al. \u0026ldquo;Concrete mathematics: a foundation for computer science.\u0026rdquo; Computers in Physics 3.5 (1989): 106-107.  ","href":"/post/repertoire/","title":"The repertoire method"},{"content":" Therer are some not-so-serious miscellaneous thoughts, comments of mine about everything in an informal format using Vietnamese and English.\nDeep Learning and truely science researching    \u0026ldquo;too often where people will something against their own gut instincts because they think the community wants them to do it that way, so people will work on a certain subject even though they aren\u0026rsquo;t terribly interested in it because they think that they\u0026rsquo;ll get more prestige by working on it. I think you get more prestige by doing good science than by doing popular science because if you go with what you really think is important then it\u0026rsquo;s a higher chance that it really is important in the long run and it\u0026rsquo;s the long run which has the most benefit to the world\u0026rdquo;\n \u0026ldquo;Do good science, not popular science.\u0026rdquo; Mình nghĩ giống Knuth, khi 1 cái gì đó quá hype (như Deep Learning chẳng hạn) thì hẳn là có gì đó sai sai ở đó. Mình có cảm giác sai sai gì đó ở AI lẫn DL (dù thực ra mình không giải thích đc) nhưng cách mọi người đổ xô vào làm mà dường như force mọi bài toán về Deep Learning khiến nó trở thành popular science hơn là \u0026ldquo;good science\u0026rdquo;. Nó phổ biến đến nổi ở Harvard có hẳn 1 term là \u0026ldquo;grad student descent\u0026rdquo;: một PhD student bỏ hàng tá thời gian đi finetune tham số cho 1 model đến khi nào nó work, hoặc beat được state-of-the-art ở 1, 2 dataset (WTF). Việc quá nhiều người bỏ thời gian công sức sức vào những work như vậy khiến cộng đồng chỉ phát triển chậm lại (theo thời gian) và hầu như không có sự đang dạng trong nghiên cứu và đóng góp tri thức con người. Deep Learning không có gì sai, chỉ là cách người ta thần thánh hóa nó trở nên rất sai. Dù sao đi nữa, chính câu nói \u0026ldquo;do good science, not popular science\u0026rdquo; chính là câu nói khiến mình cảm thấy không muốn học PhD, bởi điều mình thực sự muốn làm là \u0026ldquo;good science\u0026rdquo;, và chừng nào chưa có 1 motivation đầy đủ và nghiêm túc thì chừng đó mình vẫn chưa quyết định.\nDành cho các thím học PhD, \u0026lsquo;grad student descent\u0026rsquo; được định nghĩa và mô tả như sau (rất tốt để apply, sử dụng đc cho mọi model) A popular method for designing deep learning architectures is GDGS (gradient descent by grad student).\nThis is an iterative approach, where you start with a straightforward baseline architecture (or possibly an earlier SOTA), measure its effectiveness; apply various modifications (e.g. add a highway connection here or there), see what works and what does not (i.e. where the gradient is pointing) and iterate further on from there in that direction until you reach a (local?) optimum.\n\u0026ldquo;Thời gian trôi như chó chạy ngoài đồng\u0026rdquo; Tôi và thằng bạn hay than thở một cách hài hước những lúc thấy thời gian trôi nhanh nhưng chưa kịp làm gì là \u0026ldquo;thời gian trôi nhanh như chó chạy ngoài đồng\u0026rdquo;. Nhưng thỉnh thoảng, có những lúc thời gian trôi nhanh quá, nhanh hơn cả \u0026ldquo;con chó\u0026rdquo; kia chạy \u0026ldquo;ngoài đồng\u0026rdquo;. Nên chúng tôi quyết định tìm một tục ngữ mới sao cho tốc độ của vật thể đó còn hơn vậy.\nCó hai vấn đề cần quan tâm, đó là \u0026ldquo;con nào chạy\u0026rdquo; và \u0026ldquo;chạy ở đâu\u0026rdquo;. Ở một chừng mực nào đó, có thể chó chạy trên đồng không nhanh bằng chó chạy ngoài phố. Cũng có thể có một con vật nào đó chạy nhanh hơn chó ở trên đồng. Vấn đề tiếp theo là câu mới nó phải mang vần giống và hay như câu cũ:\nThế là chúng tôi quay về câu chuyện sinh học, bởi báo là động vật rất nhanh, nên chúng tôi muốn dùng con báo, vấn đề là báo không chạy trên đồng, cũng không chạy trên rừng hay trong đầm lầy (thực ra loài báo đốm có sống ở đầm lầy ở khu vực châu Mỹ, nhưng trong trí tưởng tượng của tôi lúc đó thì là loài báo hoa mai sống ở Châu Phi). Còn một loài báo nữa là báo săn. Với tốc độ 120 km/h thì đây là đỉnh cao của tốc độ. Vậy ta có thể tạo nên một câu với độ chính xác cao nhằm thể hiện thời gian trôi nhanh với mức độ cao nhất:\n Báo săn chạy trên xavan (trảng cỏ). Ngựa chạy trên thảo nguyên.  How to classify images of fruit  code bằng c, bắt đầu bằng việc so sánh từng pixels. Tự cảm thấy code hơi điêu, suy nghĩ, đọc phần Search trong TAoCP của Knuth. Phát hiện ra không giúp ích gì cho đời. Trong lúc rảnh google paper ở cvpr phát hiện ra phép đổi RGB -\u0026gt; xám của OpenCV lẫn matlab đều có vấn đề. Thôi thì ta tự code từ đầu đến cuối, bố dek tin đứa nào. cảm thấy manhattan distance chuối quá, chuyển sang Euclid, . Thấy trả về ảnh nào có min thấp quá cunhx hơn chuối. Quyết định trả về k ảnh, phát hiện ra kNN. Ngồi đọc sách xử lý ảnh của Gonzalez và tự hỏi cộng đồng xử lý ảnh đã lạc trôi về đâu giữa biển Deep Learning này. Thấy template matching dễ code và cài thử . Thấy template matching chạy cũng hài hài. Nản, ngồi google lung tung, nào là PCA, SVM, Tree-based methods, Deep Learning, ConvNet, Capsules net, Hinton và những người bạn, tự hỏi nên bắt đầu từ đâu. Thế là tìm cuối Computer Vision: Algorithms and its applications. Đọc và có cảm giác mình đang đọc abstract của 70 papers khác nhau chung 1 chủ đề. Tự hỏi Image Process và Computer Vision khác nhau chỗ nào? Sau CV là gì? Deep là gì? Sao dân tình lại nhà nhà người người đều deep. Ngồi nghịch tiếp kNN và template matching, phát hiện ra các kq trả về na ná nhau, táo thì nó trả về mấy thứ màu đó, còn cam thì nó trả về màu cam. Hẳn phải có cái gì đó liên quan. Tiếp tục google, thấy dân tình bảo dùng t-SNE visualize, xách code chạy thử, cũng ảo diệu dù đek hiểu nó làm gì. ok, nhìn sơ qua thì các đối tượng có màu na ná nhau thì trong cái hình t-SNE tạo ra nó nằm gần nhau (sau vài chục tiếng tune mấy tham số của t-SNE) \u0026hellip; \u0026hellip;.  Chuyện fake news, fake quotes và toxic information. Hôm nọ tình cờ phát hiện ra ông anh có quote câu:\n “If you can\u0026rsquo;t explain it to a six year old, you don\u0026rsquo;t understand it yourself.” - Einstein.\n Nhưng thực ra câu này Einstein chưa bao giờ thốt ra, cũng không có một tài liệu nào nói là ổng có đề cập đến câu đó. Câu gần nhất thì có lẽ có Feynman nói, và dựa vào đó ổng đề xuất Feynman\u0026rsquo;s method về phương pháp học tập hiệu quả.\nHiện tượng fake news (không ngờ cũng có ngày mình phải đồng ý 1 quan điểm với anh Đỗ Năm Trân), fake quotes và toxic information (đầu độc người đọc bằng các thông tin \u0026ldquo;nguy hiểm\u0026rdquo;, đặc biệt liên quan đến sức khỏe), có thể liệt kê như sau:\n Danh sách các loại thức ăn không nên ăn chung với nhau. Buồn một nỗi là mình không thể tìm thấy 1 nghiên cứu cụ thể nào nói vấn đề đó, nhưng laị được share rất nhiều trên mạng. Vì nó đề cập đến sức khỏe thành ra rất nhiều người quan tâm. Ngoài ra còn có thể loại kiểu như vụ thí nghiệm cá chết ở Fomasa (hoàn toàn phản khoa học, setting thí nghiệm muốn chửi), vụ VTV phanh phui \u0026ldquo;nước giấm làm từ hóa chất\u0026rdquo;, bản chất nếu làm từ hóa chất không có gì sai, cái sai là nguồn gốc của hóa chất đó thì không được nhấn mạnh, bởi người đọc có quan niệm \u0026ldquo;cái gì làm từ hóa chất đều độc\u0026rdquo;. Các câu danh ngôn nổi tiếng: không source, không nguồn gốc. Cả 1 đoạn trích đúng 1 câu và hiểu lệch lạc hoàn toàn vấn đề. Fake news thì có lẽ đã quá nổi tiếng ở Việt Nam. bla, bla\u0026hellip;  Trong Computer Science, có một câu cực kì kinh điển và dân tình rất thích quote: \u0026ldquo;premature optimization is the root of all evil\u0026rdquo; - Don Knuth Và bắt đầu một sự rất vui về cái quote này.\n Quote này chính Knuth nói rằng là do Tony Hoare (cha đẻ của Quicksort) nói. Nguồn gốc câu này (được dân tình cite nhiều) nằm trong paper \u0026ldquo; Structured Programming with goto Statements\u0026rdquo; - Don Knuth - Computing Surveys , 1974. Để verify quote này và tìm nguồn gốc câu trên, mình đã mua hẳn cuốn Literate Programming về đọc ==. Đọc hết cái paper ở trên, và nếu có tìm hiểu về công trình của Knuth, ta sẽ hiểu rằng có gì đó sai sai nếu Knuth là người nói câu đó: thánh ấy đã chế ra ngôn ngữ mã máy riêng, hẳn 1 cái máy tính tưởng tượng mà trong đó 1 byte = 6 bits. Với mục đích: phân tích mọi thuật toán tới từng chỉ thị và optimize mọi thứ. Có thể nói, chính thánh ấy là thánh của optimize code. Trong khi đó Hoare là cha đẻ của Verification, các biểu diễn hình thức toán học để chứng minh 1 thuật toán là đúng đắn. Và chính Hoare là người đề xuất việc \u0026ldquo;đừng cắm mặt optimize\u0026rdquo; mà hãy viết code sao cho \u0026ldquo;người khác hiểu được\u0026rdquo; và có thể chứng minh được sự đúng đắn của nó. Trong paper gốc, Knuth không cite câu trên của Hoare, nhưng trong log về TeX error ông có đề cập. Giáo sư Sedgewick (học trò Knuth, sáng lập Computer Science department ở Princeton) cũng nói trong sách Algorithms của ổng rằng câu trên của Hoare. Một điều khá hài hước là khi Hoare được hỏi là ổng có nói câu trên không, thì ông lại nói ổng tưởng Knuth nói câu đó =))  References  \u0026ldquo;For the absence of a bibliography I offer neither explanation nor apology\u0026rdquo; - A discipline of programming - Edsger W. Dijkstra.\n ","href":"/post/why_so_serious/","title":"Miscellaneous notes"},{"content":" For programming purposes, I prefer Linux to Windows. However, Windows is really good at entertaining and office works. Beside, Microsoft Office is the best office suit I ever have experienced, and OneNote is one of my favorite note-taking software. Not to mention my beloved Bilizzard games such as Diablo, Starcraft and other video games.\nA solution to integrate Windows and Linux in one machine is to install dual-boot. However, I am tired of switching between 2 OSs. In addition, we have to configure the systems so that it can easily transfer files between 2 different partition formats.\nAnother workaround is to run Windows or Linux on a virtual machine. Since I usually play video games, I could not use Windows as a client. Running Linux virtually seems promising. Unfortunately, my laptop is not strong enough to run the virtual machine smoothly.\nFinally, M$ comes up with a new feature in Windows 10 named \u0026lsquo;Windows Subsystem for Linux\u0026rsquo;. Suprisingly, it works flawlessly except several minor issues.\nSettings  Turn Windows Subsystem On: find Windows features and select Windows Subsystem for Linux , wait for the installation and restart. Open Windows Store and install Ubuntu. I also saw OpenSuse on the store. Hopefully, there will be more distros adapted in future, especially Arch Linhx/Manjaro which is my favorite.\n After working around with Ubuntu and Tmux in cmder, I have found out that wsltty is the best tool to avoid fonts broken, key arrow issues. Besides, it run faster than cmder although cmder has many useful features.\n  Here is the result:\nThere are several tips:\n Change the cursor to block. Turn off the mouse support feature in vim (:set mouse=) in order to copy text from Windows to vim. Copy from vim to Windows applications:  Install xsel/xclip on Ubuntu. Install Xming Set export DISPLAY=:0 in the bashrc or zshrc   Installing ArchLinux on WSL After several months waiting a better solution, finally I found ArchWSL which is easy to install and full of configuration. In addition, it turns out that wsltty is not a tool I really want since there are still some issues with Unicode font and delay. Now I am using xfce4-terminal with the help from Xming.\nWith a few steps we are able to run xfce4-terminal directly from Windows:\n Set Xming to run on startup. Use following command in Run to execute xfce4-terminal:  C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -windowstyle hidden -Command \u0026quot;iex \\\u0026quot;path\\to\\arch.exe run DISPLAY=:0 xfce4-terminal\u0026quot;\u0026quot;  Windows Defender It is so annoying that I have to turn it off. Somehow, the program always scans the Linux folder. Eventually, everything in Subsystem runs so slow. A better workaround is to add the Linux folder to the excluding list of Windows Defender.\n","href":"/post/ubuntu_on_windows/","title":"Why Ubuntu on Windows?"},{"content":" General  Show the information of a variable. It is very useful when we the code takes so much memory: whos \u0026lt;variable name\u0026gt;. Sum of squared of elements: sumsqr(X). Use sum(sum(A.^2, 1)) instead if you want to use less memory. Memory optimizations tips and tricks: [Undocumented Matlab] Use column-based representation instead of the row-based one. For example, to represent a list of items from 1 to 5, use [1 2 3 4 5] (not [1; 2; 3; 4; 5]). It becomes convient because MATLAB supports foreach-like statement:  for element = list % \u0026lt;statements\u0026gt; end If list is a matrix, the above statement will assign each column of list to element.\n Construct one-shot vectors from label vector matlab one_shot = full(sparse(1:length(labels), double(labels), 1));   Code Optimization  Replace isempty(find(\u0026lt;expr\u0026gt;)) by isempty(find(\u0026lt;expr\u0026gt;, 1)) to improve performance.  Libraries VLFeat  Use vl_imreadjpeg instead of imread. It could speed up reading image files.  Editor Nvim Plugins:\n vim-matlab matlab syntax  Data Structures String Find a string in a cell array find(ismember(cell_array, str));","href":"/post/matlab/","title":"Matlab"},{"content":" Compiling the libray There are many benefits of compiling the library from source instead of using the pre-compiled version:\n Supporting the latest IDE (Vision Studio). For example, pre-compiled OpenCV 3.0 (22/11/2015) is not compatible with Visual Studio 2015. Be able to customize the library, we can add several options to OpenCV, for example, we can add APIs for Kinect SDK, OpenCL, Python, etc.  Down the source code for the Windows environment here.\nStep 1: Extract the file into a folder, let say C:\\opencv.\n  Extract the library   Folder build contains pre-compiled DLLs of the library, while filder sources are used for manully installing.\n  Folders in OpenCV   Step 2: Download and install CMake.\nStep 3: Run CMake. Where is the soruce code points to folder sources of OpenCV as mentioned in the previous step. Where to build the binaries: where we put output of library, i.e, DLLs files. In this tutorial, I put it in C:\\opencv\\sources\\build.\n  CMake interface   The next step is to press Configure and later select the generator for the project. In this section, we select the corresponding IDE/Compiler. In addition, if the computer is x86 architecture then we have to select the IDE version which is x86 as well, i.e, Visual Studio 14 2015. If the computer is x64 then we select IDE versions with postfix Win64. Select Finish to complete the configuration.\nAfter that, CMake enumerate all options we can customize. In this tutorial, however, I would skip this part since the goal of this post is to install the default setting and hence other setup will be discuss in other posts. Press Generate to continue.\nAfter generating the source code with selected compiler and IDE, folder build includes different Projects/Solutions.\nStep 4: Use Visual Studio to open solution OPENCV. Beware of the version of VS have to be the same of as configured in CMAKE.\n  Targets and Architectures of Visual Studio   Next, we compile the library (press F7):\n Build the solution with the Debug mode. Build the solution with the Release mode  Each step takes about 10-15 minutes. The output maybe contains 50 successfully complied projects, 17 skipped and 2 fail projects. Visual Studio generates two particulars folders lib\\Debug and lib\\Release which contain all DLLs for Debug and Release modes, respectively.\nSetup OpenCV on Visual Studio Step 1: Create a new empty project.\n  Create a new project   Before configuring the project, we have to determine the following information for the project:\n Header files of OpenCV. Configurate for programming with OpenCV. Configurate for executing the project with OpenCV support. Since there are 2 targets, namely Debug and Release, we have to seperately configurate each target.  Header files Right click to the project InstallOpencv, select Properties (Alt + F7).\n  Config additional header file   In the sidebar, select C/C++ -\u0026gt; General -\u0026gt; Additional Include Directories, we point to opencv\\build\\include (NOT sources\\build\\include). Press OK.\nSetup the library for programming Press Alt + F7 to open dialog Properties of the Project. Select tab Linker, in Additional Library Directories we point to lib\\Debug, i.e, C:\\opencv\\sources\\bild\\lib\\Debug. Nhấn Alt + F7 để vào phần Properties của Project. Chọn Thẻ Linker, mục Additional Library Directories ta trỏ đến thư mục lib\\Debug (trong ví dụ này là: C:\\oepncv\\sources\\build\\lib\\Debug) since we are using the Debug target.\n  Configuration     Additional Library Directories   Move to tab Input, select Additional Dependencies, we input these files:\nopencv_calib3d300d.lib opencv_core300d.lib opencv_features2d300d.lib opencv_flann300d.lib opencv_hal300d.lib opencv_highgui300d.lib opencv_imgcodecs300d.lib opencv_imgproc300d.lib opencv_ml300d.lib opencv_objdetect300d.lib opencv_photo300d.lib opencv_shape300d.lib opencv_stitching300d.lib opencv_superres300d.lib opencv_ts300d.lib opencv_video300d.lib opencv_videoio300d.lib opencv_videostab300d.lib  Next, we move to the Release target and repeat previous steps. However, the path now is C:\\opencv\\sources\\build\\lib\\Release.\nAdditional Dependencies are:\nopencv_calib3d300.lib opencv_core300.lib opencv_features2d300.lib opencv_flann300.lib opencv_hal300.lib opencv_highgui300.lib opencv_imgcodecs300.lib opencv_imgproc300.lib opencv_ml300.lib opencv_objdetect300.lib opencv_photo300.lib opencv_shape300.lib opencv_stitching300.lib opencv_superres300.lib opencv_ts300.lib opencv_video300.lib opencv_videoio300.lib opencv_videostab300.lib  Setup the library for running the code While building the project, Visual Studio creates two new folders of the solution: \\Debug and \\Release with the architecture name we are using. In this example, they are x64\\Debug and x64\\Release. We copy all files from build\\bin\\Debug to x64\\Debug, also files in build\\bin\\Release to x64\\Release.\nCreate an example and run the program.\n#include \u0026lt;opencv2/opencv.hpp\u0026gt;#include \u0026lt;iostream\u0026gt;using namespace cv; using namespace std; int main() { Mat img = imread(\u0026#34;demo.jpg\u0026#34;); imshow(\u0026#34;show image\u0026#34;, img); waitKey(0); destroyAllWindows(); }  [Place demo.jpg in the project folder to run the code]\n  Demo   However There are many drawbacks here:\n Every time we create a project including OpenCV support, we have to follow these settings (Section 2). It is tedious. One mistake can cost us several hours to figure out. The size of library files in sources\\bin\\Debug and source\\bin\\Release are approximately 900MB each. Copying two libraries to the project takes 1.8GB even though the source code alone only cost several KBs. How we can get all file names of Additional Dependencies since each OpenCV version has different files.  Let see how we can resolve these problems.\nAdditional Dependencies Open the command line and point to sources\\build\\lib:\nType these commands:\ndir Debug\\*d.lib /B \u0026gt;..\\DependenciesDebug.txt dir Release\\*.lib /B \u0026gt;..\\DependenciesRelease.txt  All files are saved in DependenciesDebug.txt and DependenciesRelease.txt. We simply copy the content to Additional Dependencies when creating a new project.\nLibrary Configuration Open the Command Line with Admin permission, type these commands:\n setx -m OPENCV_DIR C:\\opencv  Windows + E to open File Explorer Nhấn Windows E để mở Windows Explore\n  File Explorer   Right click on Computer and select Properties.\n  Advanced System Settings   Select Advanced System Settings. In tab Advanced, select Environment Variables…\n  Path Variable Configurate   Fill these values: %OPENCV_DIR%\\sources\\build\\bin\\Debug;%OPENCV_DIR%\\sources\\build\\bin\\Release\nDone, henceforth, we do not have to copy files from build\\bin to the project.\nSaving configuration of the project In the sidebar Property Manager, we seethe list of all configs corresponding to target archtectures in the project.\n  Property Manager   We create a new Property Sheet by right clicking into each section and selecting Add New Project Property Sheet. Double click to the new sheet and configure as mentioned in Section 2. Visual Studio will create a new file with extension props in the project. Henceforth, while creating a new project, we only need to import the Property.\nSince we set the variable path being OPENCV_DIR, we can change the paths in the configuraton as following:\n %OPENCV_DIR%\\build\\include %OPENCV_DIR%\\source\\build\\lib\\Debug %OPENCV_DIR%\\source\\build\\lib\\Release  ","href":"/post/install_opencv_vs/","title":"Install the OpenCV library on Visual Studio"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/tags/config/","title":"Configuration"},{"content":"","href":"/tags/og/","title":"Opengraph"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/series/","title":"Series"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/tags/algorithms/","title":"algorithms"},{"content":"","href":"/tags/android/","title":"android"},{"content":"","href":"/tags/books/","title":"books"},{"content":"","href":"/tags/computer-vision/","title":"computer vision"},{"content":"","href":"/tags/courses/","title":"courses"},{"content":"","href":"/tags/history/","title":"history"},{"content":"","href":"/authors/khoa/","title":"khoa"},{"content":"","href":"/tags/math/","title":"math"},{"content":"","href":"/tags/matlab/","title":"matlab"},{"content":"","href":"/tags/misc/","title":"misc"},{"content":"","href":"/tags/notes/","title":"notes"},{"content":"","href":"/tags/opencv/","title":"opencv"},{"content":"","href":"/tags/programming/","title":"programming"},{"content":"","href":"/tags/taocp/","title":"taocp"},{"content":"","href":"/tags/visual-studio/","title":"visual studio"},{"content":"","href":"/tags/windows/","title":"windows"}]
